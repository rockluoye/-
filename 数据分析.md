### jupyter的使用

##### 1.进入jupyter

创建虚拟环境----https://pypi.douban.com/simple

在激活虚拟环境的情况下

在激活虚拟环境的情况下

安装ipython

在ipython中安装pip install jupyter

exit退出虚拟环境

cd 到要创建文件或者要打开文件的目录

注意：以.ipynb结尾的文件就是jupyter写的文件

执行jupyter notebook

在浏览器中打开jupyter提供给你的地址如：http://localhost:8888/?token=0656c234e400a84137d657332c92b86bcb3116674b7ea31f

当退出时使用快捷键ctrl + c快速按两次

新建文件，点击new下的python3 然后改名

##### 2.使用jupyter

点击文件中的内容会选中一个cell

当按a时会在这个cell的上面创建一个cell

当按b时会在cell的下面创建一个cell

快速按两下b会删掉cell

在cell中可以写自己代码，按ctrl + 回车  表示执行代码

ait + 回车表示执行完代码后会在下面继续新建一个cell

使用Markdown的语法：按esc后按m进入编辑模式

1.help()的使用：这是ipython提供的，可以查看某个语法的提示

如：help(len)

用len?可以更加快的得到帮助文档

2.tab可以自动补全

##### 3.IPython魔法命令

使用命令运行外部文件（默认是当前目录，最好加上绝对路径）：%run *.py

运行时间的计算%time  bar()

timeit会多次运行bar()函数来计算平均值

%who查看当前会话的所有变量与函数名称

%whos返回的是详细信息

列出所支持的全部魔法命令：lsmagic

4.执行当前系统的命令

如：！dir

安装三个包：！pip install numpy pandas matplotlib scipy

编辑模式的提示：Shift-Tab,提示函数的参数



数据分析三剑客：numpy，pandas，matplotlib

##### numpy，pandas和matplotlib的使用

简单的导入使用

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline #由于 %matplotlib inline 的存在，当输入plt.xxx后，不必再输入 plt.show()，图像将自动显示出来
# 查看numpy的版本
np.__version__
cat = plt.imread('./cat.jpg') # cat为像素点的三元色
plt.imshow(cat)
```

### ndarray

ndarray为封装好的矩阵，但不是真正的矩阵，可以当作为python优化的多维列表

##### 创建ndarray

```python
l = [1,3,4,5,6]
n = np.array(l) #将列表转化为矩阵
display(l, n)# 用来展示数据的
```

注意：numpy中默认所用元素的类型都是相同的

如果传进来的列表中包含不同的类型，则统一为同一类型，优先级为str>float>int

##### 使用np的routines函数创建

包含以下常见创建方法：

1) np.ones(shape, dtype=None, order='C')#shape表示这个矩阵的情况，dtype表示存储格式，np.ones表示全部为一的ndarray,order表示以行为主还是以列为主存储数组

```python
如：
np.ones(shape=(3,4), dtype=np.float32)32表示所占的大小
结果为：
array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]], dtype=float32)
```

2) np.zeros(shape, dtype=float, order='C')  全部是0的矩阵

```python
np.zeros(shape=(5,6),dtype=np.int16 )
array([[0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0]], dtype=int16
```

3) np.full(shape, fill_value=‘’, dtype=None, order='C')指定是什么值

```python
np.full(shape=(3,4),fill_value=8,dtype=np.int16)
array([[8, 8, 8, 8],
       [8, 8, 8, 8],
       [8, 8, 8, 8]], dtype=int16)
```

4) np.eye(N, M=None, k=0, dtype=float)
对角线为1其他的位置为0的矩阵，不是单位矩阵

```python
np.eye(5,6, k=0)
array([[1., 0., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0.],
       [0., 0., 1., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0.],
       [0., 0., 0., 0., 1., 0.]])
np.eye(5,6, k=1)
array([[0., 1., 0., 0., 0., 0.],
       [0., 0., 1., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0.],
       [0., 0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 0., 1.]])
当为-1时对角线往下移
```

5) np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)

将范围内的数分成num份，组成一维矩阵

```python
np.linspace(0,100,num=50)
array([  0.        ,   2.04081633,   4.08163265,   6.12244898,
         8.16326531,  10.20408163,  12.24489796,  14.28571429,
        16.32653061,  18.36734694,  20.40816327,  22.44897959,
        24.48979592,  26.53061224,  28.57142857,  30.6122449 ,
        32.65306122,  34.69387755,  36.73469388,  38.7755102 ,
        40.81632653,  42.85714286,  44.89795918,  46.93877551,
        48.97959184,  51.02040816,  53.06122449,  55.10204082,
        57.14285714,  59.18367347,  61.2244898 ,  63.26530612,
        65.30612245,  67.34693878,  69.3877551 ,  71.42857143,
        73.46938776,  75.51020408,  77.55102041,  79.59183673,
        81.63265306,  83.67346939,  85.71428571,  87.75510204,
        89.79591837,  91.83673469,  93.87755102,  95.91836735,
        97.95918367, 100.        ])
np.linspace(0,100,num=50， endpoint=False)    endpoint表示不显示后面的100
array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22., 24.,
       26., 28., 30., 32., 34., 36., 38., 40., 42., 44., 46., 48., 50.,
       52., 54., 56., 58., 60., 62., 64., 66., 68., 70., 72., 74., 76.,
       78., 80., 82., 84., 86., 88., 90., 92., 94., 96., 98.])
np.linspace(0,100,num=50,endpoint=False,retstep=True)
retstep表示返回元组，并显示间距为2.0
(array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22., 24.,
        26., 28., 30., 32., 34., 36., 38., 40., 42., 44., 46., 48., 50.,
        52., 54., 56., 58., 60., 62., 64., 66., 68., 70., 72., 74., 76.,
        78., 80., 82., 84., 86., 88., 90., 92., 94., 96., 98.]), 2.0)
```

6) np.arange([start, ]stop, [step, ]dtype=None)  与python中的range一样，一般用在整数中

```python
np.arange(0, 100, 2)#左闭右开

array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32,
       34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66,
       68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98])
```

7) np.random.randint(low, high=None, size=None, dtype='l')

```python
np.random.randint(0,150,size=(3,4,5))
产生一个随机的3维3个二维的4行5列的整数的矩阵
array([[[ 23,  22, 145, 136, 138],
        [ 55,   6,  74,  84,  11],
        [  7, 125, 139,  25,  37],
        [139,  14, 104,  66,  13]],

       [[135, 138,  56,  42,  84],
        [ 19,  50,  99,  28,  23],
        [ 45,  19, 134, 149,  14],
        [121,  64,  65,  23, 136]],

       [[108,  75, 143, 145,  38],
        [ 49,  11,  32, 146,  29],
        [ 25,  67,   8,  68,  58],
        [ 54,  56,  77, 147,  87]]])

```

8)np.random.normal(loc=0.0, scale=1.0, size=None)

正太分布

```python
np.random.normal(loc=10, scale=3, size=(10, 3))#loc为平均值，scale方差，10表示有10行，每行3个
array([[12.09059127,  6.81862478,  8.43082926],
       [ 6.0593108 , 14.22221792,  2.25924591],
       [ 7.13724326,  6.28579828,  7.6196867 ],
       [ 9.69809002, 11.2365869 , 13.33621692],
       [11.18351475,  8.21590875, 10.88402463],
       [ 9.7245853 , 14.51210762,  8.87299844],
       [11.75011501,  7.03392745,  8.16900554],
       [ 3.80150983,  5.01646701, 10.83741606],
       [16.50179397, 13.26947994, 14.79273538],
       [10.61400249, 15.35798737,  6.72549871]])
np.random.normal(loc=10, scale=3, size=(10, 3)).mean()显示平均值
np.random.normal(loc=10, scale=3, size=(10, 3)).std()方差
```

9) np.random.randn(d0, d1, ..., dn)

标准正太分布：平均值为0，方差为1的正太分布

```python
np.random.randn(2,3,4)
array([[[-0.58445145, -0.83789091, -0.9168369 , -0.66654059],
        [ 0.83686215,  0.12815292,  0.47642119, -0.16528136],
        [-1.81799615, -2.10089479, -0.63079765, -0.07416868]],

       [[-0.43859656,  1.34301521,  0.33364185, -0.05961628],
        [ 1.05960047,  1.13036711,  1.76372176, -0.41736931],
        [-0.36749069,  0.06462155,  0.16878902, -0.86582313]]])
```

10) np.random.random(size=None)

生成0到1的随机数，左闭右开

```python
np.random.random(size=(1,20,3))
3表示几维
20表示二维数组个数
1表示有二维数组中有几个数组
array([[[0.02229711, 0.52593169, 0.5955573 ],
        [0.7359475 , 0.086919  , 0.01575395],
        [0.02472348, 0.17005046, 0.57820752],
        [0.62662445, 0.57298625, 0.26582879],
        [0.1674688 , 0.80549671, 0.5588716 ],
        [0.50981494, 0.8257917 , 0.17606425],
        [0.2213972 , 0.78618258, 0.12876429],
        [0.4805465 , 0.24484097, 0.10612908],
        [0.29439282, 0.09330974, 0.9915758 ],
        [0.0406746 , 0.54717233, 0.3082143 ],
        [0.25129503, 0.08075311, 0.41891417],
        [0.88897288, 0.81469137, 0.07465413],
        [0.04471721, 0.41212953, 0.50367726],
        [0.6519798 , 0.87644136, 0.71665199],
        [0.61831234, 0.85327107, 0.64697588],
        [0.7292345 , 0.37152865, 0.69767226],
        [0.8176755 , 0.17551744, 0.69225776],
        [0.20229321, 0.44734184, 0.39036694],
        [0.33943017, 0.37293382, 0.06819525],
        [0.78246733, 0.14343088, 0.08330387]]])
```

np.random.rand(1,20,3)与np.random.random(size=None)一样

```python
# 小总结
np.ones():创建一个数值为一的数组
np.zeros():创建数值为0的数组
np.full():创建指定填充数值的数组
np.eye():创建单位矩阵
np.random.rand():创建一个均匀分布的随机数组，数值范围为0~1
np.random.randint(start,stop，step,size): 创建一个以start开始stop结束步长为step的均匀数组
np.random.uniform(start,stop，step,size):指定选取一定范围内随机数的数组
np.arange(start, stop, step):指定数值在一定范围内的一维数组
np.linspace(start, stop, step):指定数值在一定范围内的一维数组
np.random.normol(均值，标准差，维度)：创建一个正太分布
np.random.randn(2,3,4):创建一个标准正太分布既平均值为0，方差为一
```

##### ndarray的属性

ndim：维度，shape：形状（给维度长度），size：总长度，dtype 元素类型

#### ndarray的基本操作

##### 索引

```python
n = np.random.randint(0,100,size=(3,4,2))
n[1,2,0]表示第二个中的第三个的第一个元素
```

##### 切片

```python
n = np.random.randint(0,100,size=(3,4,2))
n[1:3] 从一切到三个
n[1:3， 1：3]切完第一个维度，对第二个维度进行切割
```

##### 数据反转

n[::-1]将第一维度数据数据反转

n[::-1, ::-1]将第一维度和第二维度数据反转

n[:,::-1]第二维度数据反转

##### 变形

```python
使用reshape函数
n = np.random.randint(0,100,size=(4,5))
array([[59, 85, 42, 76,  8],
       [14, 17,  6, 19, 91],
       [15, 34, 14, 48, 64],
       [63, 49, 57, 70,  3]])
n.reshape((5,4))或n.reshape(5,4)或np.reshape(n,newshape=(5,4))
array([[18, 55, 59, 34],
       [35,  8, 63, 13],
       [75, 28, 77, 16],
       [ 2, 97, 13, 97],
       [91, 70, 20, 69]])
```

##### 级联

垂直级联:vstack

np.concatenate((n1, n2),axis=0)要求n2一致

水平级联:hstack

np.concatenate((n1, n2),axis=1)要求n1一致

##### 切分

np.split(n, [1,3])从1切到3，分为三个矩阵左闭右开

np.split(n, [1,3]，axis=1)表示对列切

深拷贝：n.copy()

#### 聚合操作

求和：np.sum

n.sum():所有元素相加的和

n.sum(axis=1)每一行的和

n.sum(axis=0)每一列的和

axis表示维度

最大值np.max，最小值np.min

np.argmin(n, axis=1)返回最小值的索引，对行的操作

np.any(n)矩阵中有一个为真返回True

np.all(n)矩阵中有一个为假返回False

np.nan :空数据：not a number

np.nansum(n):当数据中有np.nan时把它当作0计算

##### 使用pandas打开操作文件

read_csv/read_table函数的参数

path：表示文件系统位置，url,文件型对象的字符串

sep或delimiter：用于对行中各字段进行拆分的字符串序列或正则表达式

```python
data = pd.read_table('Z:/test.txt',header=None,encoding='gb2312',sep=',',index_col=0)
```

read_csv   	to_csv/txt
read_excel	to_excel
read_hdf	to_hdf
read_sql  	to_sql
read_json	to_json
read_html	to_html
read_stata	to_stata
read_clipboard	to_clipboard
read_pickle	to_pickle
read_msgback	to_msgback

read_bbq	to_gbq



##### 矩阵的操作

1.算数运算

2.矩阵运算

广播机制

1.为缺失的维度补一

2.假定缺失的元素用已有的值填充

display(n1,n2)

##### 排序

快速排序np.sort()不改变原始数据

ndarray.sort()本地处理，并不占用空间，但改变原始数据

#### 图片的处理

```python
array([[[231, 186, 131],
        [232, 187, 132],
        [233, 188, 133],
        ...,
        [100,  55,  52],
        [ 92,  48,  49],
        [ 85,  43,  44]],

       [[232, 187, 132],
        [232, 187, 132],
        [233, 188, 133],
        ...,
        [100,  55,  52],
        [ 92,  48,  49],
        [ 84,  42,  43]],

       [[232, 187, 132],
        [233, 188, 133],
        [233, 188, 133],
        ...,
        [ 99,  54,  51],
        [ 91,  47,  48],
        [ 83,  41,  42]],

       ...,

       [[199, 119,  82],
        [199, 119,  82],
        [200, 120,  83],
        ...,
        [189,  99,  64],
        [187,  97,  62],
        [187,  97,  62]],

       [[199, 119,  82],
        [199, 119,  82],
        [199, 119,  82],
        ...,
        [188,  98,  64],
        [188,  95,  62],
        [188,  95,  62]],

       [[199, 119,  82],
        [199, 119,  82],
        [199, 119,  82],
        ...,
        [188,  98,  64],
        [188,  95,  62],
        [188,  95,  62]]], dtype=uint8)
以上中每一个二维数组表示每一行像素点
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline 

np.__version__
cat = plt.imread('./code/cat.jpg') # cat为像素点的三元色
plt.imshow(cat)
对数组操作：
颜色颠倒：
cat1 = cat[:,:,::-1]
plt.imshow(cat1)
左右颠倒
cat2 = cat[:,::-1]
plt.imshow(cat2)
上下颠倒
cat3 = cat[::-1]
plt.imshow(cat3)
降低精度：
cat4 = cat[::4,::4]
plt.imshow(cat4)
打马赛克
cat5 = cat.copy()
cat5[50:100,125:150]=0
plt.imshow(cat5)
```

#### Series的创建：数据必须是一维的

字典创建方式：

```python
from pandas import Series
s = {"语文":150,"数学":150,"英语":150,"理综":300}
s1 = Series(s)
s1
```

第二种方式：

```python
s1 = Series(data=[150]*3 + [300], index=['语文','数学','英语','理综'])
s1
```

列表或数组创建：

```python
from pandas import Series
s = Series([1,2,4,6,9,33,22,11,99])
s
```

##### 修改

```python
s[1] = 100
s
0      1
1    100
2      4
3      6
4      9
5     33
6     22
7     11
8     99
dtype: int64
```

##### 显式 索引

1.s["数学"]

2.s.loc["数学"]#推荐写法返回的是值

3.s1.loc[["数学"]]返回的是series

##### 隐式索引:使用系统给定的下标

s.iloc[1]

##### 切片

显式 切片，全闭区间

s1.loc["语文":"英语"]

```python
语文    150
数学    150
英语    150
dtype: int64
```

隐式切片左闭右开

s1.iloc[2:3]

英语    150
dtype: int64

##### Series的基本概念

可以看成是有序的字典

通过shape,size,index,values得到有序的series属性

s.shape：形状

s.index:所有索引

s.head(3)头3行，默认为前三行

s.tail(3)尾 3行 

s.isnull();判断元素是否缺失,有缺失返回True

s.notnull判断元素是否缺失,有缺失返回False

或pd.notnull(s)

##### 运算

1.与单个数字的运算:s + 1 所有元素加一

2.series与series的运算

相同索引相加，不同补nan

3.有缺失的运算

s.add(s2,fill_value=0)#有缺失的补0

```python
s = {"语文":150,"数学":150,"英语":150,"理综":300}
s1 = Series(s)
s3 = {"语文":150,"数学":np.nan,"英语":150,"理综":300,"文综":300}
s2 = Series(s3)
s1 + s2
s1.add(s2,fill_value=0)


数学    150.0
文综    300.0
理综    600.0
英语    300.0
语文    300.0
dtype: float64

```

series没有广播机制，没有的相加为nan

```python
from pandas import Series
s = {"语文":150,"数学":150,"英语":150,"理综":300}
s1 = Series(s)
s3 = {"语文":150,"数学":150,"英语":150,"理综":300,"文综":300}
s2 = Series(s3)
s1 + s2

数学    300.0
文综      NaN
理综    600.0
英语    300.0
语文    300.0
dtype: float64
```

#### DataFrame

创建：

第一种创建方式

```python
from pandas import DataFrame
import numpy as np
df = DataFrame(data=np.random.randint(0,150,size=(4,4)),index=["张三","李四","王五","赵六"],columns=["语文","数学","英语","理综"])
df
```

第二种：

```python
df = DataFrame({'语文':np.random.randint(0,150,size=4),'数学':np.random.randint(0,150,size=4)},index=['张三','李四','王五','赵六'])
```

##### 属性

df.values

df.columns:列索引

df.index:行索引

df.shap:n*n

##### 列索引

1.df.loc['语文']

2.df.语文

3.df[['语文']]   返回的是DataFrame

新增一列数据

df['计算机'] = np.random.randint(0,150,size=4)

##### 行索引

显式索引

df.loc['张三']

df.loc[['我', '路人甲']]#取多个

隐式索引

df.iloc[0]

#### 元素索引

##### 先行后列

1.df.loc['李四'].loc['语文'] #链式索引

2.df.loc['李四‘,'语文']

3.df\['语文']['李四']

##### 先列后行

1.df['语文'].loc['李四']

2.df.loc[:,'李四'].loc['语文']

取多个列

df.loc[:,('语文','数学')]

##### 修改

dd4.loc['张三','英语'] = np.nan   可以修改

dd4.loc\['张三']['英语'] = None   不能修改成功

##### 行切片

1.df['李四':'王五']

2.df.loc['李四':'王五']

隐式索引切片：

df.iloc[1:3]#左闭右开

##### 列切片

df.loc[:,'数学':'语文']

df[['数学','英语']]

##### 运算

df1 + df2 行列索引一致，对应的元素相加，不相同的索引为nan

df.add(df3,fill_value=0)  缺失的默认为一，再相加

##### Series与DataFrame之间的运算

1.

s = Series(index=['语文','数学','英语,'理综'],data=np.random.randint(0,150,size=4))

Series与DataFrame之间，默认对比的是列索引，所以为列相加

2.

使用python操作符：以行为单位操作（参数必须是行），对所有行都有效

使用pandas操作涵数：

axis=0：以列为单位操作（参数必须是列），对所有列都有效

axis=1：以行为单位操作（参数必须是行），对所有行都有效

### 处理缺失数据

有两种缺失数据

。None

。np.nan(NaN)

np.nan是浮点类型

##### numpy

当矩阵中有nan数据时，求和出来的数为nan

可以用np.nansum(n)此时会把nan视为0再相加

#### pandas中none和nan的操作

##### 1.判断函数

isnull()：缺失返回True

notnull()：缺失返回False

ddd.isnull().any(axis=1):按每一行判断

ddd.isnull().any()：按每一列判断

##### 2.过滤函数

dropna()：默认对行操作，当行中有NaN时会删掉整行

dropna(axis=1)对列操作，当列中有NaN时会删掉整列

ddd.dropna(how='all',subset=['数学','英语'],inplace=True)在数学英语列中有nan的行会被删掉,表示会修改原始数据

##### 3.填充函数

fillna(value=88)#用指定的值去填充nan

可以选择表中填充方式：有backfill，bfill，pad，ffill:向前填充

ddd.fillna(method='ffill', axis=1,limit=1)    limit:表示连续填充次数为1，默认不修改原数据

对于DataFrame来说axis=0：index行，axis=1：columns列

##### drop():删除元素

ddd.drop(index=['李四,'王五'])：删除李四行和王五行

ddd.drop(columns=['语文,'数学'])：删除语文列和数学列

ddd.drop(labes='语文')

##### none和nan的区别：

none是python中的一个类型，是object类型，不能参与计算

nan是float类型，是numpy用来表示not a number的一种情况

nan可以参与运算，但是结果总是为nan，如果想正常运算，使用np.nan*()

pandas中nan和none都表示为nan



### 多层索引

#### 1.隐式构造

##### DataFrame

```python
import numpy as np
from pandas import DataFrame
from pandas import Series
import matplotlib.pyplot as plt
index = [['一班','一班','一班','二班','二班','二班'],['张三','李四','王五','张三','李四','王五']]
columns = [['期中','期中','期中','期末','期末','期末'],['语文','数学','英语','语文','数学','英语']]
data = np.random.randint(0,150,size=(6,6))
df = DataFrame(data=data,index=index,columns=columns)
df
```

##### Series

```python
s = Series(index=index,data=np.random.randint(0,150,size=6))
s
```

#### 2.显式构造

##### DataFrame

1.使用数组

```python
index1 = pd.MultiIndex.from_arrays([['一班','一班','一班','二班','二班','二班'],['张三','李四','王五','张三','李四','王五']])
df1 = DataFrame(data=data,index=index1,columns=columns)
df1
```

2.使用元组

```python
index2 = pd.MultiIndex.from_tuples([('一班','张三'),('一班','李四'),('一班','王五'),('二班','张三'),('二班','李四'),('二班','王五')])
data = np.random.randint(0,150,size=(6,6))
df2 = DataFrame(data=data,index=index2,columns=columns)
df2
```

3.使用product     推荐使用

```python
index3 = pd.MultiIndex.from_product([['一班','二班'],['张三','李四','王五']])
df3 = DataFrame(data=data,index=index3,columns=columns)
df3
```

#### 多层列索引

多层索引原则不能直接索引内层索引

##### Series的操作

显式索引：s.loc['一班','张三']-----------显式索引不能直接对内层索引

隐式索引：s.iloc[0]

df3.iloc[[0,1,3,4],[]]---第一个【】指的是行第二个【】指的是列

切片

显式索引切片：s.loc['一班':'二班']---显式切片不能直接对内层切片

隐式索引切片：s.iloc[0:4]

##### DataFrame的操作

列索引

```python
df3['期末']
df3['期末']['语文']
df3['期末','语文']
```

行索引

```python
df3.loc['二班','张三']
```

链式调用：df3['期末','语文'].loc['一班','张三']

行切片

```python
df['一班':'二班']
隐式切片
df.iloc[0:3]
```

隐式列切片

df.iloc[:,0:4]

修改：数据类型不一样不能赋值

修改数据类型

df3.astype(np.float32)

##### 索引的堆

1.stack()

df3.stack()默认将最内层的一个列索引变为行索引

df3.stack(level=0)指定0层的列索引变为行索引

2.unstack()

df3.unstack()默认将最内层的一个行索引变为列索引

df3.unstack(level=0)指定0层的行索引变为列索引

##### 当索引中没有值时补nan

可以指定补的值为0：df3.unstack(fill_value=0)

##### 聚合操作

ddd.sum()，ddd.max()，ddd.min()，ddd.mean()：默认对行操作

在聚合操作中先看axis，axis=0表示对行操作，axis=1表示对列操作再看level，level=1指定那个层级保留，如：

```python
df3.sum(level=0,axis=0)# 默认axis=0不保留层级

期中	期末
语文	数学	英语	语文	数学	英语
一班	291	177	224	247	185	225
二班	164	183	212	279	235	288
```

求张三李四各个科目中的最高分：df3.iloc[[0,1,3,4],].max(axis=1,level=0)

#### 数据的读取

```python
import matplotlib.pyplot as plt
%matplotlib inline
apple = pd.read_csv('./data/AAPL.csv')
apple
```

1.检查是否有缺失数据：apple.isnull().any()

2.检查数据类型：apple.dtypes

3.将object数据类型的转化如时间

to_datetime()将字符串的日期格式转化为pandas的日期格式

apple['Date'] = pd.to_datetime(apple['Date'])

4.将某一列数据设置为行索引

apple.set_index(keys='Date')

5.重置行索引

apple.reset_index()

6.绘制图形：选择Adj Close字段绘制折线图

#默认以Series的行索引作为x轴数据，values数据作为y轴数据

apple['Adj Close'].plot(figsize=(12,9))   figsize=(12,9)指图像的大小

#### pandas的拼接操作

快速生成DataFrame结构的函数

```python
def make_df(index,cols):
    df = DataFrame({col:[col + str(i) for i in index] for col in cols})
    return df
df = make_df([1,2,3,4],list('ABCD'))
```

##### 1.使用pd.concat()级联：匹配级联

pd.concat((df1,df2))#默认增加行数

pd.concat((df1,df2),axis=1)#增加列数

pd.concat((df1,df2),ignore_index=True) 忽略原来索引，增加默认索引

pd.concat((df1,df2),keys=['df1','df2']) 变成层级索引，如：

```python

	A	B	C	D
df1	0	A1	B1	C1	D1
    1	A2	B2	C2	D2
    2	A3	B3	C3	D3
    3	A4	B4	C4	D4
df2	0	A1	B1	C1	D1
    1	A2	B2	C2	D2
    2	A3	B3	C3	D3
    3	A4	B4	C4	D4
```

##### 2.不匹配级联

外连接：不匹配补nan

```python
pd.concat((df1,df3),sort=True)#默认行追加
	A	B	C	D	E
0	A1	B1	C1	D1	NaN
1	A2	B2	C2	D2	NaN
2	A3	B3	C3	D3	NaN
3	A4	B4	C4	D4	NaN
0	NaN	B2	C2	D2	E2
1	NaN	B3	C3	D3	E3
2	NaN	B4	C4	D4	E4
3	NaN	B5	C5	D5	E5

pd.concat((df1,df3),sort=True,axis=1) # 设置列追加
	A	B	C	D	B	C	D	E
0	A1	B1	C1	D1	NaN	NaN	NaN	NaN
1	A2	B2	C2	D2	B1	C1	D1	E1
2	A3	B3	C3	D3	B2	C2	D2	E2
3	A4	B4	C4	D4	B3	C3	D3	E3
4	NaN	NaN	NaN	NaN	B4	C4	D4	E4
```

内连接；只连接匹配项

```python
pd.concat((df1,df3),sort=True,axis=1,join='inner')
	A	B	C	D	B	C	D	E
1	A2	B2	C2	D2	B1	C1	D1	E1
2	A3	B3	C3	D3	B2	C2	D2	E2
3	A4	B4	C4	D4	B3	C3	D3	E3
```

左连接：只连接左边的匹配项

```python
pd.concat((df1,df3),sort=True,axis=1,join_axes=[df1.index])
	A	B	C	D	B	C	D	E
0	A1	B1	C1	D1	NaN	NaN	NaN	NaN
1	A2	B2	C2	D2	B1	C1	D1	E1
2	A3	B3	C3	D3	B2	C2	D2	E2
3	A4	B4	C4	D4	B3	C3	D3	E3
```

右连接：只连接右边的匹配项

```python
pd.concat((df1,df3),sort=True,axis=1,join_axes=[df3.index])
	A	B	C	D	B	C	D	E
1	A2	B2	C2	D2	B1	C1	D1	E1
2	A3	B3	C3	D3	B2	C2	D2	E2
3	A4	B4	C4	D4	B3	C3	D3	E3
4	NaN	NaN	NaN	NaN	B4	C4	D4	E4
```

##### append()函数添加

append是Dataframe对象的方法，pd没有append方法

```python
df1.append(df2,verify_integrity=False)#verify_integrity检查是否有重复，有重复报错
	A	B	C	D
0	A1	B1	C1	D1
1	A2	B2	C2	D2
2	A3	B3	C3	D3
3	A4	B4	C4	D4
0	A1	B1	C1	D1
1	A2	B2	C2	D2
2	A3	B3	C3	D3
3	A4	B4	C4	D4
```

##### pd.merge()合并：merge既是对象也是pd方法

```python
fff1 = DataFrame({'name':['张三','李四','Chales'],'id':[2,3,4],'age':[22,21,25]})
fff2 = DataFrame({'sex':['男','男','女'],'id':[2,3,4],'group':['sale','search','service']})
display(fff1,fff2)
```

一对一：

```python

	name id	age
0	张三	2	22
1	李四	3	21
2	Chales	4	25
	sex	id	group
0	男	2	sale
1	男	3	search
2	女	4	service
```

pd.merge(fff1,fff2)

```python
   name	id	age	sex	group
0	张三	2	22	男	sale
1	李四	3	21	男	search
2	Chales	4	25	女	service
```

一对多：只有相同的才显示

```python
	name id	age
0	张三	1	22
1	李四	2	21
2	Chales	2	25
	sex	id	group
0	男	2	sale
1	男	3	search
2	女	4	service
```

pd.merge(fff1,fff2)

```python

	name	id	age	sex	group
0	李四	2	21	男	sale
1	Chales	2	25	男	sale
```

Series的unique()：去重函数

多对多：会形成笛卡尔积

```python

	name id	age
0	张三	1	22
1	李四	2	21
2	Chales	2	25
	sex	id	group
0	男	2	sale
1	男	2	search
2	女	4	service


	name id	age	sex	group
0	李四	2	21	男	sale
1	李四	2	21	男	search
2	Chales	2	25	男	sale
3	Chales	2	25	男	search
```

存在多个列相同的时候，需要我们指定那个列合并

pd.merge(df1,df2,on='name')

当没有列相同的时候，需要分别指定需要合并的列

pd.merge(df1,df2,left_on='name',right_on='名字')

##### 内合并

pd.merge(df1,df2,left_on='name',right_on='名字') #默认只显示合并的数据，叫做内合并

##### 外合并

pd.merge(df1,df2,left_on='name',right_on='名字',how='outer')

##### 左合并

pd.merge(df1,df2,left_on='name',right_on='名字',how='left')

##### 右合并

pd.merge(df1,df2,left_on='name',right_on='名字',how='right')

##### 列冲突解决

pd.merge(df1,df2,on='name',suffixes=['\_df1','\_df2'])----指定添加后缀

unique():去重函数------n.unique()-----只对Series使用

query() 条件查询函数-----n.query("name == 'Lucy' & age>14")

查询的方法：temp['name'] == 'Lucy'    返回数组

显示查询所有：temp[temp['name'] == 'Lucy']

#### pandas数据处理

##### 1.删除重复元素

```python
import numpy as np
from pandas import DataFrame
from pandas import Series
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
def make_df(index,cols):
    df = DataFrame({col:[col + str(i) for i in index] for col in cols})
    return df
df = make_df([1,2,3,4],list('ABCD'))
df.loc[1] = df.loc[0]
	A	B	C	D
0	A1	B1	C1	D1
1	A2	B2	C2	D2
2	A3	B3	C3	D3
3	A4	B4	C4	D4
# 检测是否有重复的行
df.duplicated()# keep='last'时从后往前检查

0    False
1     True
2    False
3    False
dtype: bool
# 指定检查的是那几列
df.duplicated(subset=['B','C','D'])
0    False
1     True
2    False
3    False
dtype: bool

# 删除重复元素
df.drop_duplicates()
	A	B	C	D
0	A1	B1	C1	D1
2	A3	B3	C3	D3
3	A4	B4	C4	D4

# 逻辑运算
np.logical_not(df.duplicated())
0     True
1    False
2     True
3     True
dtype: bool


df[~df.duplicated()]
	A	B	C	D
0	A1	B1	C1	D1
2	A3	B3	C3	D3
3	A4	B4	C4	D4
```

##### 新增一列

 df.insert(0,'政治',df2['政治']) ：0代表列名数字索引，政治：新增列名，

df2['date'] = date 默认插入最后一列

##### 映射

1.replace()函数：替换元素

2.map()函数：新建一列

3.rename()函数：替换索引

```python
df = DataFrame(data=np.random.randint(0,150,size=(4,4)),index=["张三","李四","王五","赵六"],columns=["语文","数学","英语","理综"])
#定义一个字典
mapping = {
    1:100,3:130,7107
}
df.replace(mapping,inplace=True)# 可以用字典中的value替换表中与字典中key相同的数，默认不修改原始数据，修改要加inplace=True
#简单使用map()
mapping = {
    0:130,77:107
}
df['computer'] = df['理综'].map(mapping)

	语文	数学	英语	理综	computer
张三	136	76	125		93	NaN
李四	61	7	97		77	107.0
王五	115	53	86		79	NaN
赵六	0	23	23		47	NaN
#map()中使用函数
def convert(score): # score为df['语文']的一个值，循环传入所有值后结束
    if score <90:
        return '不及格'
    elif score<120:
        return '良好'
    else:
        return '优秀'
df['语文_评分'] = df['语文'].map(convert)

	语文	数学	英语	理综	computer 语文_评分
张三	136	76	125		93	NaN		优秀
李四	61	7	97		77	107.0	不及格
王五	115	53	86		79	NaN		良好
赵六	0	23	23		47	NaN		不及格

#rename():替换索引
mapping = {'语文':'Chinese','数学':'Math','英语':'English'}
df.rename(mapper=mapping,axis=1)
	Chinese	Math	English	理综	computer	语文_评分
张三	136		76		125		93	NaN			优秀
李四	61		7		97		77	107.0		不及格
王五	115		53		86		79	NaN			良好
赵六	0		23		23		47	NaN			不及




df.party.value_counts() #查看party列中相同的元素出现的次数
# 将某段时间字符串转为自定义字符串
def transform_date(date):
    day,month,year = date.split('-')
    mon = months[month]
    return str(mon)+'-'+str(day)+'-'+str(year)
df['contb_receipt_dt'] = df['contb_receipt_dt'].map(transform_date)
# 将字符串时间转化为pandas时间数据类型
df['contb_receipt_dt'] = pd.to_datetime(df['contb_receipt_dt'])

# 对contb_receipt_dt进行排序，拿到排序后的索引列表
take_list = df['contb_receipt_dt'].sort_values().index

# 再使用新的索引列表重新排布election_data
sort_data = df.take(take_list)
```

##### 易忘记

df['政治']与df1['政治']赋值时索引必须相同

链式调用不能修改元数据，copy()后修改，赋值给原数据

df['数学'] = df['数学'].astype('float') # 强行修改原数据类型

df2.insert(0,'date',date) # 将某一列插入到指定位置

##### 异常值的检测和过滤

df.describe()：查看每一列的描述性统计量

```python
		语文			数学			英语		理综	computer
count	4.000000	4.000000	4.00000		4.000000	1.0
mean	78.000000	39.750000	82.75000	74.000000	107.0
std		60.844063	30.782842	43.08422	19.356308	NaN
min		0.000000	7.000000	23.00000	47.000000	107.0
25%		45.750000	19.000000	70.25000	69.500000	107.0
50%		88.000000	38.000000	91.50000	78.000000	107.0
75%		120.250000	58.750000	104.00000	82.500000	107.0
max		136.000000	76.000000	125.00000	93.000000	107.0
```

使用std()函数可以求得DataFrame对象的每一列的标准差

```python
df.abs() > 3 * df.std() #绝对值大于3倍标准差的显示true
	  语文	数学		英语		理综
张三	False	False	False	False
李四	False	False	False	False
王五	False	False	False	False
赵六	False	False	False	False
```

##### 随机抽样：使用.take()函数

```python
df.take([1,2,3,0],axis=0)#使用列表中的索引将行排序或列
	语文	数学	英语	理综
李四	9	129	4	148
王五	84	36	86	16
赵六	80	145	134	81
张三	41	77	29	108
```

无放回抽样

```python
df.take(np.random.permutation([0,1,2,3]))
	语文	数学	英语	理综
赵六	80	145	 134	81
李四	9	129	 4		148
王五	84	36	 86		16
张三	41	77	 29		108
```

无放回抽样

```python
df.take(np.random.randint(0,4,size=4))
	语文	数学	英语	理综
张三	41	77	 29	  108
赵六	80	145	 134	81
王五	84	36	 86		6
王五	84	36	 86		6
```

#### 数据聚合

```python
# 第一步分组
df.groupby(by='color').groups
# 第二步 聚合，在分组的基础之上进行聚合
df.groupby(by='color').sum()
#多个分组
df.groupby(['party','contb_receipt_dt']).sum()
# 查看某一字典的聚合
# 第一种 先对所有的数据进行聚合，然后再从聚合中的结果取出price
df.groupby(by='color').sum()[['price']]
# 第二种 先从分组中取出数据，然后进行聚合，推荐写法
df.groupby(by='color')[['price']].sum()
# 第三步 将聚和的结果，合并到原始表数据中
df.merge(price_sum,left_on='color',right_index='True',suffixes=['','_sum'])
#left_index:使用左边数据表的行索引
#right_index:使用右边数据表的行索引
```

字典方法创建DataFrame

```python
ddd = DataFrame({'item': ['萝卜', '萝卜', '萝卜', '白菜', '白菜','辣椒','辣椒','辣椒','冬瓜', '冬瓜'], 'color': ['白', '青', '红',  '白', '青', '青', '红', '白','白', '青'],
                'weight': [50, 60, 70, 80, 60, 40, 100, 20, 120, 100], 'price': [1.98, 2.98, 5.98, 1.28, 2.58, 6.98, 8.88, 12.8, 0.8, 1.1]})
ddd
```

##### 高级数据聚合

使用transform和apple实现相同功能

### 绘图函数

##### 线性图

用来反映数据的趋势

简单Series图

s = Series(data=np.random.randint(0,10,size=10))

s.plot()

简单的DataFrame图

默认行作为x轴，列作为y轴

```python
df = DataFrame(data=np.random.randint(0,150,size=(4,4)),index=["张三","李四","王五","赵六"],columns=["语文","数学","英语","理综"])
df.plot()
```

##### 柱状图

用来反映数据的对比大小

```python
df.plot(kind='bar')
# 将数据转化为比例
sum1 = df.sum(axis=1) #求每行的总和
#df/sum1 #默认除法在列中进行
result = df.div(df.sum(axis=1),axis=0) #使用div函数除法，自定义axis
result.plot(kind='bar')
# 水平条形图
result.plot(kind='barh')
```

##### 直方图

用来展示数据的概率密度分布

```python
df.plot(kind='hist')
df.plot(kind='kde') # 数据走势
```

##### 散点图

放映数据分布位置

```python
df.plot('A','B',kind='scatter')
#两两数据之间形成的矩阵
pd.plotting.scatter_matrix(df.figsize=(16,16),alpha=0.5)
```

第五天
	图片灰度处理
		1.使用平均值
j = plt.imread('jinzhengen.png')  导图
j_mean = j.mean(axis=2)
plt.figure(figsize=(12,9))
plt.imshow(j_mean, cmap='gray')
		2.使用最大值：
j_max = j.max(-1)
plt.figure(figsize=(12,9))
plt.imshow(j_max, cmap='gray')
		3.使用加权平均值
j_weight = np.dot(j, [0.299,0.587, 0.114])
plt.imshow(j_weight, cmap='gray')
	scipy
		文件的输入输出
			存入：
import scipy.io as spio
moon = plt.imread('/data/moonlanding.png')
spio.savemat('./moon.mat',{'moon':moon})
			读取：
spio.loadmat('./moon.mat')
			图片的读取：
cat = Image.open('./data/cat.ipg')
cat
			滤镜：
cat.filter(ImageFilter.BLUUR)    虚化
cat.filter(ImageFilter.CONTOUR) 轮廓
cat.filter(ImageFilter.DETAIL)  细节强化
cat.filter(ImageFilter.EDGE_ENHANCE)  边缘强化
			图片处理：
face = misc.face()
plt.figure(figsize=(10,8))
plt.imshow(face)
				shift移动坐标：ndimage.shift(face,(200,300,0))
ndimage.shift(face,(200,300,0),mode='mirror')  镜像
ndimage.shift(face,(200,300,0),mode='reflect') 反射
				旋转：ndimage.rotate(face,90)
				缩放：ndimage.zoom(face,(0.6,0.8))
				切割图片：face[270:480,500,800]
				高斯滤波：ndimage.gaussian_filter(face_noise,sigma=1)
				中值滤波：ndimage.median_filter(face_noise,size=1)
				维纳滤波：signal.wiener(face_noise,mysize=1)
				添加噪声：face + face.std() * np.random.randn(*face.shape)
	Matplotlib基础知识
		只含有单一曲线的图：x = np.linspace(-10, 10, 1000)
y = np.sin(x)
plt.plot(x,y)
-10，10表示x轴的范围，1000表示函数所取的函数中的点的数量
		包含多个曲线的图
			一个图多条线：x = np.linspace(-1,1,1000)
plt.plot(x,(1-x**2)**0.5)
plt.plot(x,x)
			多个图：x = np.linspace(-1,1,1000)
plt.plot(x,x)
plt.show()
plt.plot(x,np.cos(x))
			在一个plot中传入多个值：x = np.linspace(-5,5,1000)
plt.plot(x,x*3,x,(1-x**2)**0.5, x, np.cos(x))
		网格线
			x = np.linspace(-10,10, 1000)
y = np.cos(x)
plt.plot(x,y)
plt.grid(b=True, axis='y', color='red', linestyle='--', linewidth=5)#y表示网格线在y轴上，color='red'表示线为红色，linestyle表示线的形状，linewidth表示线宽
		画子图
			pit.figure(figsize=(3 * 3,4)) #总宽为6高位4
axes = plt.subplot(1,3,1) # 表示一行三列第一个图
axes.plot(x,np.sin(x))
axes2 = plt.subplot(1,3,2)# 表示一行三列第二个图
axes.plot(x,np.cos(x))
		坐标界限
			plt.axis('equal')表示长宽的单位长度是一样长
			plt.axis() #不传任何值表示，返回当前坐标系的上下界限
			plt.axis([xmin,xmax,ymin,ymax]) # 指定xy最大最小界限
			plt.axis('off') #将坐标轴关闭
			plt.axis('scaled') # 表示xy界限刻度一样
			单独对xy操作
plt.ylim(ymin=-1,ymax=1)
plt.xlim(xmin=-1,xmax=1)
			设置xy轴标签
xlabel = plt.xlabel('X',fontdict=dict(fontsize=20),position=(1,0))
xlabel.set_color('red') # 设置颜色
y轴类似
			标题
				plt.title('The Circle') # 设置属性与标签类似
		图例：图例的名称不能以下划线开头
			第一种方法：
plt.plot(x,np.sin(x),label='sin')
plt.legend()
			第二种方法：
plt.plot(x,np.sin(x))
plt.legend(['sin',])
			loc参数
				plt.legend(['sin',],loc='upper right')
				plt.legend(['sin',],loc=(0,1)) 使图例在0，1坐标处
			ncol参数：控制图例有多少列
				plt.legend(['sin',],loc=(0,1),ncol=2) # 表示两列 
		保存图片
			figure = plt.figure()
plt.plot(x,np.sin(x))
plt.legend(['sin'])
figure.savefig('./sin.png',dpi=200,facecolor='red') #dpi表示画布颜色,facecolor画板颜色
	设置plot的风格和样式
		函数曲线颜色设置
			plt.plot(x,np.sin(x),c='#0000ff')
			plt.plot(x,np.sin(x),c=(0.3,0.3,0.6))
			plt.plot(x,np.sin(x),color='red')
		函数曲线透明度设置
			plt.plot(x,np.sin(x),color='red',alpha=0.5)
		画布的背景色
			plt.subplot(facecolor='r')
		画板的背景色
			plt.figure(facecolor='g')
		函数线型
			plt.plot(x,np.sin(x),ls=':')
plt.plot(x,np.sin(x),ls='-')
plt.plot(x,np.sin(x),ls='None')
		函数线宽
			plt.plot(x,np.sin(x),lw=3)
		不同长度的虚线,元素个数必须是偶数
			plt.plot(x,np.sin(x),dashes=[3,2,5,2;])
		点型
			plt.plot(x,np.sin(x),marker='1',markersize=50) #markersize点大小
		多参数连用:颜色，点型，线型
			plt.plot(x,np.sin(x),"dr--",markersize=50)
		X，Y坐标刻度
			plt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi], ['0', '$\pi/2$', '$\pi$', '$3\pi/2$', '2$\pi$']) # 第一个列表是刻度值，第二个是刻度标签名
			面向对象的方法
				axes = plt.subplot()
axes.plot(x,  np.sin(x))
axes.set_xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi]) # 设置刻度
axes.set_xticklabels(['0', '$\pi/2$', '$\pi$', '$3\pi/2$', '2$\pi$']) # 设置刻度标签
	条形图
		# 条形图必须要传x,y的数据.
n = np.random.randint(0,10,  size=10)
plt.bar(np.arange(0,10,), n, width=0.8,color='r')
	直方图
		data = [1,2,34,5,6,78,12]
plt.hist(data,density=True) # 表示概率
plt.hist(data,orientation='horizontal',color='r') #水平图
	饼图
		未占满饼图
			n  = [0.3, 0.2, 0.3] # 列表中填的是比例且相加不是100%就表示未占满
plt.pie(n)
		占满饼图
			n = [1,2,3,4]
plt.pie(n,autopct='%.2f%%') # 显示比例
			plt.pie(n,autopct='%.2f%%', pctdistance=0.8) # 表示离圆心的距离
			plt.pie(n,autopct='%.2f%%', pctdistance=0.8,labels=list('ABCD')) # 为比例写标签 
			plt.pie(n,autopct='%.2f%%', pctdistance=0.8,labels=list('ABCD'),labeldistance=1.2) #比例标签离圆心距离 
			plt.pie(n,autopct='%.2f%%', pctdistance=0.8,labels=list('ABCD'),labeldistance=1.2,shadow=True) # 有阴影
			plt.pie(n,autopct='%.2f%%', pctdistance=0.8,labels=list('ABCD'),labeldistance=1.2,shadow=True,explode=[0.1, 0.2, 0.3,  0.2]) # 爆炸图，列表中的值表示与圆心距离 
			plt.pie(n,textprops=dict(fontsize=20, color='r')) # 设置标签文字格式
			plt.pie(colors=['r', 'g', 'b', 'y']) 为饼图配色
	散点图
		plt.scatter(x,y,c='r',s=100,marker='*') # s 为点大小,marker点型

##### 创建极坐标条形图

1.将风向次数统计

使用np.histogram

data,degree = np.histogram(ssss,range=[0,360],bins=8)

2.生成极坐标系

plt.figure(figsize=(8,8))

plt.gca(projection='polar',facecolor='g',)

degree = np.arange(0,2*np.pi,np.pi/4)

plt.bar(degree,data,color=np.random.rand(8,3))

使用for循环导入数据的方法：

```python
# 方法一
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
%matplotlib inline
import glob
from pandas import DataFrame
cities = DataFrame()
for file in glob.glob('../*.csv'):
    df = pd.read_csv(file)
    cities = cities.append(df)# 将所有数据拼接到一个DataFrame中
cities.shape
# 方法二
[file for file in os.listdir() if file.endwith('.csv')]
# 方法三
import re
[file for file in os.listdir() if re.match(r'[a-z]+_\d{6}\.csv',file)]
# 方法三
for i,j,k in os.walk('./'):
    # i为当前目录，j为当前目录下的目录，k为当前目录下的所有文件
	defgsdhfjhsgd
```

DataFrame条件查询方法：

```python
dist_near = dists[dists < 100]
dist_far = dists[dists > 50]
display(dist_near,dist_far)
# dists < 100 返回的是true和false 只要索引相同即可引用
temp_near = temps_max[dists < 100]
temp_far = temps_max[dists >= 50]
```

玫瑰图画法

```python
# 得到数据
speeds = []
for i in range(45,361,45):
    speed = milano.loc[(milano['wind_deg'] >= i - 45) & (milano['wind_deg'] < i),'wind_speed'].mean()
    speeds.append(speed)
speeds

# 画图
plt.figure(figsize=(8,8))
plt.gca(projection='polar',facecolor='g',)
degree = np.arange(0,2*np.pi,np.pi/4)
plt.bar(degree,speeds,color=np.random.rand(8,3))
```

##### Scipy文件的输入输出

```python
文件的保存
import scipy.io as spio
spio.savemat('./moon.mat',{'moon':moon})
文件读取
spio.loadmat('./moon.mat')
```



##### 机器学习分类：

有监督学习：分为

1.分类

k近邻，朴素贝叶斯，决策树，SVM

2.回归

线性回归，逻辑回归，岭回归

无监督学习：

聚类：K-means

半监督学习

深度学习

#### knn算法

安装包：!pip install sklearn

knn算法原理：KNN的原理就是当预测一个新的值x的时候，根据它距离最近的K个样本点是什么类别来判断x属于哪个类别

KNN算法的优点：精度高，对异常值不敏感，无输入假定；

缺点： 时间复杂度高，空间复杂度高

适用数据：数值型和标称型

解决方法：

1.对已知样本点进行剪辑，事先去除对分类作用不大的样本点

2.使用加权平均

##### 简单调包：分类

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplolib inline
# 导包
from sklearn.neighbors import KNeighborsClassifier
# 导数据
movie = pd.read_csv('./movie')
# 做散点图
plt.scatter(movie['武打'],movie['镜头'])
# 实例化knn
knn = KNeighborsClassifier()
# 数据训练
knn.fit(X_train,y_train)
# 数据预测
y_ = knn.predict(X_test)
```

切割数据

```python
from sklearn.model_selection import train_test_split
# data要被切分的数据，target切割对应的标记，test_size切多少，0.2表示比例，30表示30条
target = df['salary'] # 要预测的y值
X_train,X_test,y_train,y_test = train_test_split(data,target,test_size=0.2)
```

计算得分

knn.score(X_test,y_test)

坐标矩阵的生成方法

X，Y= numpy.meshgrid()——生成网格点坐标矩阵

将多维数组转化为一维数组：X.ravel()

np.c_[X.ravel(),Y.ravel()]将两个矩阵的行拼接成一个新矩阵

当两个矩阵都为一维矩阵时两个的元素当作一行，生成二维数组

```python
# item为data['occupation']字段中的值
def convert(item):
    index = np.argwhere(occupations==item)[0,0]
    return index
data['occupation'] = data['occupation'].map(convert)
```



##### 简单调包：用于回归

```python
X_train = np.random.rand(80) * 10
y_train = np.sin(x_train)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
# 训练
knn.fit(X_train.reshape(-1,1),y_train)
# 预测
y_ = knn.predict(X_test)
# 画折线图与散点图
plt.scatter(X_train,y_train)
plt.plot(X_test,y_,c='r')
# 算得分
knn.score(X_train.reshape(-1,1),y_train)
```

#### 线性回归

原理：分类的目标变量是标称型数据，，而回归将会对连续型的数据做出预测

枚举函数的使用：可以用在字典和列表中使用

```python
for i,feature in enumerate(feature_name):
    print(i,feature)
```

##### 简单调包

```python
# 以简单的糖尿病数据为例
# 1.导线性回归的包
from sklearn.linear_model import LinearRegression
# 2.导糖尿病的数据
from sklearn.datasets import load_diabetes
diabetes = load_diabetes()
data = diabetes['data']
target = diabetes['target']
# 导DataFrame包显示数据
from pandas import DataFrame
df = DataFrame(data=data, columns=feature_name)
df
# 选取数据的特征
feature_name = diabetes['feature_names']
# 训练数据和测试数据的获取
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(data,target,test_size=0.2)
# 用训练数据 训练模型
linear = LinearRegression()
linear.fit(X_train,y_train)
# 显示模型的系数
linear.coef_
# 显示模型的截距
linear.intercept_
# 对测试数据进行预测
y_ = linear.predict(X_test)
y_
# 计算训练数据得分
linear.score(X_train,y_train)
# 计算测试数据得分
linear.score(X_test,y_test)
# 将各个字段与结果target的关系用散点图展示
plt.figure(figsize=(3*4,4*4.5))
for i,feature in enumerate(feature_name):
    # 画散点图
    axes = plt.subplot(4,3,i+ 1)
    X_train = df[[feature]]
    axes.scatter(X_train,target)
    axes.set_title(feature)
    # 数据训练
    linear = LinearRegression()
    linear.fit(X_train,target)
    # 随机获取数据
    X_test = np.linspace(X_train.min(),X_train.max(),100).reshape(-1,1)
    # 预测数据
    y_ = linear.predict(X_test)
    # 画线性图
    axes.plot(X_test,y_,c='r',lw=2)
    axes.set_title(feature + '%s'%(linear.score(X_train,target)))
```



#### 岭回归

原理：使用缩减系数来理解数据，简单来说，岭回归就是在矩阵X

```python
简单来说岭回归就是在矩阵XTXXTX上加一个λI从而使得矩阵奇异，进而能对XTX+λIXTX+λI求逆，其实I是一个n*n的单位矩阵，对角线全为1，其他元素全为0，而λ是一个用定义的数值，那么回归系数就变成了：w^=(XTX+λI)−1XTyw^=(XTX+λI)−1XTy 
岭回归是对最小二乘回归的一种补充，它损失了无偏性，来换取高的数值稳定性，从而得到较高的计算精度。 
岭回归最先用来处理特征数对于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计，这里通过引入λ来限制所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫缩减。

```

from sklearn.linear_model import Ridge

#### lasso回归

from sklearn.linear_model import Lasso

##### Logistic回归

对分类的边界线建立回归公式，

只能做分类

机器学习算法选择：先逻辑回归再用复杂的算法

逻辑回归的决策边界：可以是非线性的

优缺点：

优：实现简单，易于理解和实现：计算代价不高，速度很快，存储资源低

缺：容易欠拟合，分类精度可能不高

```python
from sklearn.linear_model import LogisticRegression
logistic = LogisticRegression()
logistic.fit(data,target)
x, y = np.linspace(data[:,0].min(),data[:,0].max(),1000), np.linspace(data[:,1].min(),data[:,1].max(),1000)
X,Y = np.meshgrid(x,y)
XY = np.c_[X.ravel(),Y.ravel()]
y_ = logistic.predict(XY)
```

##### 决策树：简单调包

可以做分类，也可以做回归

分类调包

```python
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(max_depth=2) # max_depth指定决策树深度来防止过拟合
# 设置切分时树的子节点最少位2，左节点最少为1,来防止过拟合
tree = DecisionTreeClassifier(min_samples_split=4,min_samples_leaf=2)
tree.fit(data,target).score(data,target)
```

回归调包

```python
from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor()
w = np.linspace(-100,100,100)
x = np.cos(w)
y = np.sin(w)
dt.fit(w.reshape(-1,1),np.c_[x,y])
X_test = np.linspace(-100,100,200).reshape(-1,1)
y_ = dt.predict(X_test)
y_
```

##### 随机森林

```python
随机有2层含义
1.随机抽样，所有的树抽的样本数是一样
2.特征随机，特征数必须一样
如果是分类的话，最后的结果，投票
如果是回归的话，最后的结果，取平均
```

简单分类调包

```python
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=100) # 随机森林中树的个数n_estimators
rfc.fit(data,target)
rfc.score(data,target)
rfc.feature_importances_ # 为特征重要性
```

##### 贝叶斯调包

```python
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
ga_nb = GaussianNB()
from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer()
tf.fit(data)
data_tf = tf.transform(data)
data_dense = data_tf.toarray()
```

小整数：【-5,256】

#### SVM

svm主要是针对小样本数据进行学习，分类和预测（有时也叫回归）的一种方法，能解决神经网络不能解决的过学习问题

分类问题

简单的调包

一般的核函数

```python
from sklnear.svm import SVC
model = SVC(kernel='linear')
model.fit(data,target)
# 第二种
from sklearn.svm import LinearSVC
Linear = LinearSVC()
linear.fit(data,target)
y_linear = Linear.predict(XY)
Linear.score(data,target)
```

多项式核函数

```python
from sklnear.svm import SVC
model = SVC(kernel='ploy')
model.fit(data,target)
y_poly = ploy.predict(XY)
```

加入径向基函数既高斯核函数也叫基于半径的核函数

```python
from sklearn.svm import SVC
clf = SVC(kernel='rbf',C=1E6) # C为参数，需要自己设定，当C趋近于很大时：意味着分类严格不能有错误，当C趋近很小时：意味着可以有更大的错误容忍
clf.fit(x,y)
```

回归调包

```python
from sklearn.svm import SVR
linear = SVR(kernel='linear')
linear.fit(X_train.reshape(-1,1))
```

画出分类图

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import scipy.io as spio
data1 = spio.loadmat('./data/SVM/ex6data1.mat')
data1['X']
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
methods = {
    'linear':SVC(kernel='linear'),
    'LinearSVC':LinearSVC(),
    'rbf':SVC(kernel='rbf',C=1E6)
}
methods['linear'].fit(data1['X'],data1['y'])
# 获取支持向量的点
sv = methods['linear'].support_vectors_
sv
y_predict = methods['linear'].predict(XY)
methods['linear'].score(data1['X'],data1['y'])
# 提取系数获取斜率
w1,w2 = methods['linear'].coef_[0,0], methods['linear'].coef_[0,1]
# 获取截距
b = methods['linear'].intercept_[0]
#平面方程为
# z = x*w1 + y*w2 + b
# 当平面映射下来时z=0
# 所以
# y = -w1/w2 * x - b/w2
plt.scatter(data1['X'][:,0],data1['X'][:,1],c=data1['y'].ravel())
plt.scatter(sv[:,0],sv[:,1],alpha=0.6,c='g',s=300)
x = np.linspace(0,4,100)
y = -w1/w2 * x - b/w2
plt.plot(x,y,c='r')

# 画上边界与下边界
b_up = -3.3575*w2 - w1*1.5841
plt.scatter(data1['X'][:,0],data1['X'][:,1],c=data1['y'].ravel())
plt.scatter(sv[:,0],sv[:,1],alpha=0.6,c='g',s=300)
x = np.linspace(0,4,100)
y = -w1/w2 * x - b/w2
plt.plot(x,y,c='r')
plt.plot(x,-w1/w2 * x - b_up/w2,c='g')
```

##### K-Means原理

K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大

K-Means评估指标--轮廓系数

```python
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
silhouette_score()
data = pd.read_csv('')
X_train = data.iloc[:,1:]
kmeans = KMeans(3)
kmeans.fit(X_train)
labels = kmeans.labels_ # 这个可以认为是kmeans的真实值
silhouette_score(X_train,labels) # 计算得分
scores = []
for k in range(2,20):
    kmeans = KMeans(k)
    kmeans.fit(X_train)
    labels = kmeans.labels_ # 这个可以认为是kmeans的真实值
    score = silhouette_score(X_train,labels) # 计算得分
    scores.append(score)
# 以k为x轴数据，scores为y轴数据
plt.plot(range(2,20),scores)
plt.xticks(range(2,20),range(2,20))
plt.xlabel('k')
plt.ylabel('silhouette coefficient')
plt.grid()
# 计算每一个样本的轮廓系数
from sklearn.metrics import silhouette_samples
kmeans = KMeans(3)
kmeans.fit(X_train)
labels = kmeans.labels_
silhouette_samples(X_train,labels)
s.mean() # 轮廓系数计算出来的平均值，就是得分

# 数据预处理之标准化
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler_X_train = scaler.fit_transform(X_train)
scaler_X_train.mean() # 标准化的平均值接近于0
scaler_X_train.std() # 标准化的方差接近于1
```

##### 网格搜索

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
iris = load_iris()
data = iris['data']
target = iris['target']
for k in range(2, 20):
    knn = KNeighborsClassifier(n_neighbors=k, ) # n_neighbors：取邻近点的个数k。k取1-9测试
    knn.fit(data, target)
    score = knn.score(data, target)
    print(k, score)
    from sklearn.model_selection  import GridSearchCV
    from sklearn.model_selection import train_test_split
    knn = KNeighborsClassifier()
param_grid = {
    'n_neighbors': [3, 4, 5, 6,7],
    'weights': ['uniform', 'distance'],
    'p': [1,2]
}
gv = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5) # 将数据分为6类，以最后一类作为预测数据不动，4类作为特征，1类作为预测结果，交叉验证
gv.fit(data, target)
gv.best_params_ # 找出n_neighbors最佳为6
gv.best_estimator_ # 最好的参数设置
gv.predict(data)
gv.best_score_ # 最好的得分

# 交叉验证
from sklearn.model_selection  import KFold, StratifiedKFold # KFold为普通交叉验证，StratifiedKFold为随机交叉验证
kf = KFold(5) # 交叉验证分5格
for train, test in skf.split(data, target):
    print(train, test)
    print(knn.fit(data[train], target[train]).score(data[train], target[train]))
    knn.predict(data[test])
```

两组数据的相关关系使用协方差来表示：若协方差为负，数据关系为负相关，若为正为正相关。如果为0表示两组数据没有关系

##### PCA降维

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.decomposition import PCA
digits = pd.read_csv('../digits.csv')
digits.head()
digits.shape
data = digits.iloc[:,1:]
target = digits.label.values
# 不绛维
from sklearn.svm import SVC
svc = SVC()
# svc.fit(digits, target)
# 不降维,计算量太大,耗时太长.
# 不绛维,效果不一定好.

# 去降维
# n_components 目标维度
# whiten = True, 把数据的标准偏差变成一样.
pca = PCA(n_components=30, whiten=True) # 计算出特征值和协方差矩阵来进行降维
data_pca = pca.fit_transform(data)
data_pca.shape
svc = SVC()
svc.fit(data_pca, target).score(data_pca, target)# 训练与计算得分
# pca的好处,减少特征数,减少训练的时间,降低算法复杂度,
# 坏处,失去了数据原来的物理意义.
```

##### 人脸识别技术

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV,train_test_split
# lfw labeled face wild
from sklearn.datasets import fetch_lfw_people
# 使用fetch_lfw_people导入数据，如果本地没有会从网络下载，如果本地有数据加载本地数据
faces = fetch_lfw_people(resize=1,min_faces_per_person=70)# min_faces_per_person表示每个人脸的个数
faces
data = faces['data']
target = faces['target']
data.shape

pca = PCA(n_components=300,whiten=True)
pca.fit(data)
data_pca = pca.transform(data)
# 从11750特征（像素）提取了300个关键的特征（高纬度-------->低维度）
data_pca.shape
data_pca
# 网格搜索找到最佳方法
c = [0.2,0.5,1,3,5] # c越小表示错误容忍度越大
kernel = ['rbf','poly']
degree = [1,3,5,7]
# If gamma is 'auto' then 1/n_features will be used instead.1/300 = 0.003
gamma = [0.001,0.003,0.005,0.01,0.1]
svc = SVC()
gCV = GridSearchCV(svc,param_grid={'C':c,'kernel':kernel,'degree':degree,'gamma':gamma})
gCV.fit(data_pca,target) # 训练数据
gCV.best_score_ # 最好的得分
gCV.best_params_ # 最好的参数
svc_best = gCV.best_estimator_ # 最好的训练方法
# 预测数据
y_ = svc_best.predict(X_test)
# 8行6列显示
plt.figure(figsize=(6*3,8*4))
for i in range(48):
    axes = plt.subplot(8,6,i+1)
    axes.imshow(X_test[i].reshape(125,94),cmap='gray')
    axes.axis('off')
    t = target_names[y_test[i]].split(' ')[-1]
    p = target_names[y_[i]].split(' ')[-1]
    if y_test[i] != y_[i]:
         axes.set_title('True:%s\nPredict:%s'%(t,p),fontdict=dict(fontsize=15,color='r'))
    else:
          axes.set_title('True:%s\nPredict:%s'%(t,p))
```

在网上查找图片使用机器学习模型进行预测

```python
bush = plt.imread('./bush.jpg')
plt.imshow(bush)
bush.shape
# 灰度化处理
gray = [0.299,0.587,0.114]
bush_gray = np.dot(bush, gray) / 3
plt.imshow(bush_gray, cmap='gray')
bush_head = bush_gray[70: 340, 130: 310]
plt.imshow(bush_head, cmap='gray')
bush_head.shape
import scipy.ndimage as ndimage
x = 125 / 270
y = 94 / 180
bush_head_zoomed = ndimage.zoom(bush_head, zoom=(x,y))# 缩放比例，zoom为比例
plt.imshow(bush_head_zoomed, cmap='gray')
bush_head_zoomed.shape
bush_head_zoomed = bush_head_zoomed.reshape(1, -1) 
bush_head_zoomed.shape
bush_pca = pca.transform(bush_head_zoomed) # 降维
svc_best.predict(bush_pca) # 预测
输出为：array([3], dtype=int64)
target_names[3] # 显示
```

##### 特征工程

定义：数据中抽取出来的对结果预测有用的信息

工作应用：工作中70%的时间处理数据，30%的时间建模，模型状态评估

大部分人的工作：1.跑数据，2.数据清洗，3.业务分析，分析case，找特征，4.LogisticRegressor逻辑斯蒂回归

重要环节：数据格式化，确定存储格式

数据清洗：

1.错误数据：如身高5米，需要删除

2.组合或统计属性判定：如判断是否会买篮球鞋，而样本中女性样本占85%

3.补齐可对应的缺省值，不可信样本丢弃，缺省极多的样本不考虑

数据采样：不能失去平衡

正负样本量很大，对量的样本随机采样，使其达到一定比例

正负样本量不大：

1.采集更多的样本

2.oversampling，硬生生增加量少的一方，容易过拟合

3.修改损失函数，增加量大的惩罚权重

#### 机器学习评价指标

##### 1.AUC

定义：是一个模型评价指标，用于二分类模型的评价，AUC是指曲线下的面积的英文缩写c表示的曲线为ROC曲线，中文叫做受试者曲线

真实类别0，预测类别0 为TN

真实类别1，预测类别0为FN

真实类别0，预测类别1为FP

真实类别1，预测类别1为TP

True Positive Rate=TP/(TP+FN),代表将真实正样本划分为正样本的概率 为真阳率

False Positive Rate=FP/(FP + TN),代表将真实负样本划分为正样本的概率  伪阳率

接着以True Positive Rate=TP/(TP+FN)为y轴，False Positive Rate为x轴画ROC曲线

阈值为是选取的概率值：当概率大于阈值表示为1，小于表示为0

精确率：Precision=TP/（TP + FP）------真阳率，比如：

我预测有十个患病的而我只对了7则精确率为7/10

召回率：Recall = TP/（TP + FN）：十个患病的人让我预测，我只对了7个，召回率为7/10

F-measure:F-measure = (2\*Precision*Recall )/(Precision+Recall )

准确率：Accuracy = （TP + TN）/（TP + TN + FP + FN），就是得分，既预测对的百分比

#### tensorflow框架

1.Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，Tensorflow即为张量从图的一端流动到另一端

2.支持CNN(卷积神经网络)，RNN(循环神经网络)和LSTM(长短期记忆网络)算法，是目前在Image，NLP最流行的深度神经网络模型

##### 优点

1.用c++写的源码，提供了python的接口，写的很快，具有可读性

2.在CPU和GPU系统上都可以运行

3.代码编译效率高

4.社区发展的非常迅速

5.能生成显示网络拓扑结构和性能的可视化图

tensorflow的三种结构：变量，常量，占位符

##### 1.定义常量

常量：就是a= 1赋值后，a不能再改变了，在python中是没有常量的

```python
import tensorflow as tf
# 定义常量
hello = tf.constant('hello world')
# 在tensorflow中执行代码需要在session中
# 新建session
session = tf.Session()
# 使用session运行代码
session.run(hello).decode()
# session用完之后需要关闭
session.close()
# 在关闭后不能再使用sessioni运行代码，若要运行需要再新建session
# 为了不每次使用session后都要手动关闭session，推荐使用with
with tf.Session() as sess:
    print(sess.run(hello))
```

##### 2.定义变量

```python
# initial_value 初始值
# dtype 变量的类型
b = tf.Variable(initial_value=1,dtype=tf.int32)
# 变量在使用之前,一定要先进行初始化.
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print(sess.run(b))
    print(tf.assign(b, 20))
```

##### 3.变量的赋值

```python
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print(sess.run(b))
    # 给变量赋值的操作
    c = tf.assign(b,30)
    print(sess.run(c))
b = b + 10 # 可以这样赋值，但改变了b的类型
with tf.Session() as sess:
    sess.run(init)
    print(sess.run(b)) # 可以输出
    # 会报错
    with tf.Session() as sess:
    sess.run(init)
    print(sess.run(b)) # 可以输出
    # 给变量赋值的操作
    c = tf.assign(b,30) # 不能赋值，会保存
    print(sess.run(c))
```

##### 4.加法，减法操作

```python
add = tf.assign_add(b,20)
sub = tf.assign_sub(b,5)
with tf.Session() as sess:
    sess.run(init)
    print(sess.run(b))
    print(sess.run(add))
    print(sess.run(sub))
# 结果为
1
21
16
```

##### 基本数学运算

```python
const1 = tf.constant(1)
const2 = tf.constant(2)
add = const1 + const2
sub = const1 - const2
with tf.Session() as sess:
    print(sess.run(add))
    print(sess.run(sub))
    
矩阵的运算
import numpy as np
const1 = tf.constant(np.array([[1,2,3],[2,3,4]]))
const2 = tf.constant([[1,2],[2,3],[3,4]])
matmul = tf.matmul(const1,const2)
with tf.Session() as sess:
    print(sess.run(matmul))
```

##### 5占位符

```python
a = tf.placeholder(dtype=tf.float64)
b = tf.placeholder(dtype=tf.float64)
add = tf.add(a, b)
with tf.Session() as sess:
    print(sess.run(add,feed_dict={a:1,b:2}))
```

