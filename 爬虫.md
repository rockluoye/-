### 爬虫基础过程

1.导包：import urllib.request

2.打开首页并返回

```python
import urllib.request

url = 'https://www.baidu.com/'
#请求百度首页并返回数据
res = urllib.request.urlopen(url=url)
```

取响应的表头：

```python
print(res.headers)
```

获取响应某个字段：如Etag

```python
print(res.headers.get('Etag'))
```

读取

res.read()---------这个读取出来的是二进制

res.read().decode()-----读取所有返回并解码成unicode

res.readlines()-----与read类似，读取所有的返回变成一个列表

创建请求对象去请求

```python
url = 'https://www.baidu.com'
headers = {
    "Accept": "text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01",
    "Accept-Encoding": " gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": " no-cache",
    "Connection": "keep-alive",
    "Cookie": "BAIDUID=8C85A5C9E78B725FD53096E0047040CE:FG=1; BIDUPSID=8C85A5C9E78B725FD53096E0047040CE; PSTM=1557752450; BD_UPN=12314553; COOKIE_SESSION=2_0_7_6_0_2_0_0_6_2_1_0_1560413828_1560413728_0_0_0_0_1560739458%7C9%230_1_1560413720%7C1; H_PS_645EC=59f1YS%2Fi3hVTBC8x57xDGq6uke%2F6S7ARUjw3IIpzuKeM2OvCyFVKDO7N1E8; delPer=0; BD_HOME=0; H_PS_PSSID=29272_1464_21101_29135_29238_28519_29099_29368_28831_29220",
    "Host": "www.baidu.com",
    "Pragma": "no-cache",
    "Referer": "https://www.baidu.com/",
    "User-Agent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36",
}
#创建请求对象，headers为浏览器中复制过来的请求头对象
request = ullib.request.Request(url=url, headers=headers)
#打开request对象，并取得其返回值---值为二进制
res = urllib.request.urlopen(request)
#读取返回的值，并解码，返回给html
html = response.read().decode()#返回的json字符串
```

load和loads的区别

json.loads()解码python json格式，json.loads()是用来读取字符串的
json.load()加载python json格式文件，json.load()是用来读取文件

序列化的处理

```python
html = response.read().decode()  # 返回的json字符串

movies_dict = json.loads(html)  # 进行序列化处理
```

取字典中的值

```python
movies_list = movies_dict.get('subjects')  # 取字典中关键字为subjects的value，用get时当没有key时，也不会报错
```

字符串的格式化：

格式化的字符串文字前缀为’f’和接受的格式字符串相似str.format()

```python
url = f'https://movie.douban.com/j/search_subjects?type=tv&tag=%E7%83%AD%E9%97%A8&page_limit=50&page_start={50 * page}'
```

关于post的请求

```python
import json
import urllib.request
import urllib.parse

url = f'https://music.163.com/weapi/v1/resource/comments/R_SO_4_1367368790?csrf_token=&'

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36",
}

data = {
    'params': 'J72yQwTo7M1kjmDO4BCLpzTzOzb26Wu7zGArFTdT+Qbfp2EiLY+2+zL+4XHEXUVSGQf6SSFmPXMgIycg19UClreV5lI7kOkyzTg/jpOId91Qyd5yr2zP5eeJf8rr4mh1euIShc5CkDfm2bJR+zoitTMqxFbeA9AlkyFIiAsjxfZrpKqmPTmLKJz2XEwWyBJk',
    'encSecKey': 'b0d2dc20b6fbdd4ae834b76bb277a135a4a26d5ba330ce0119b659cec1312d9d0b3522154b0c9cb16c5b34e618aab0a0e79590c1460e2cad3cbfcccbc02ab90fff7cdde1b4e6571ec5b13f7d23c615f5da49e632d38e20f8f36e31cfa43ce37eb0f6fa178509b7c8a7bb014d4900cb4b9fbc6ed831d7f3626dce975d582d6819'
}

data = urllib.parse.urlencode(data).encode()

# 创建request对象并传入参数，传入data后，request请求方式会自动变成post请求
request = urllib.request.Request(url=url, headers=headers, data=data)

# 获取评论
response = urllib.request.urlopen(request)

json_str = response.read().decode()  # 解码返回json

users_dict = json.loads(json_str)

# 取到普通评论
comments = users_dict.get('comments')

# 火爆的评论
hotComments = users_dict.get('hotComments')
#循环打印评论
for comment in comments:
    users_dict = comment.get('user')
    nickname = users_dict.get('nickname')
    content = comment.get('content')
    print(f'{nickname}:{content}')

for comment in hotComments:
    users_dict = comment.get('user')
    nickname = users_dict.get('nickname')
    content = comment.get('content')
    print(f'{nickname}:{content}')
```

图片的保存

```python
import urllib.request

urllib.request.urlretrieve('https://www.baidu.com/img/bd_logol.png', 'baidu.png')
```

清除缓存

urllib.request.urlcleanup()

正则匹配：re.findall(匹配规则,html,re.S)

去除换行\n:re.sub('\n', '', html)

替换：local = local.replace('/', '')

字符串前面加f，表示格式化输入

```python
url = f'https://movie.douban.com/j/search_subjects?type=tv&tag=%E7%83%AD%E9%97%A8&page_limit=50&page_start={50 * page}'
```

reg = re.compile('<h2>(.*?)</h2>', re.S):返回一个正则匹配

reg.findall()

strip():去除字符串中的空格，换行

##### 打开器：build_opener

```python
http_opener = urllib.request.build_opener(cookie_handler)
```

##### 操作器：HTTPHandler

```python
http_handle = urllib.request.HTTPHandler(debuglevel=1)#debuglevel=1为调试等级
```

使用打开器：

url = '<https://www.baidu.com/>'

res = http_opener.open(fullurl=url)

res.read().decode()

打开器可以使用自定义操作器

http_opener = urllib.request.build_opener(http_handle)

#### cookie应用

##### 1.获取cookie

```python
import urllib.request
from http import cookiejar
#创建一个饼干盒，用来存放cookie
my_cookiejjar = cookiejar.CookieJar()
#创建一个cookie的操作器
cookie_handler = urllib.request.HTTPCookieProcessor(my_cookiejjar)
#创建一个打开器，来加载cookie的操作器
opener = urllib.request.build_opener(cookie_handler)
url = 'https://www.baidu.com/'
#用打开器打开
res = opener.open(url)
html = res.read().decode()
for cookie in my_cookiejjar:
    print(cookie.name, cookie.value, cookie.domain, cookie.expires)#name为cookie的key值，domain是cookie相关的域名，expires为过期时间
```

MozillaCookieJar：当打开url时将cookie文件保存在filename的文件中

```python
import urllib.request
from http import cookiejar

my_cj = cookiejar.MozillaCookieJar(filename='baidu.txt')
m_handler = urllib.request.HTTPCookieProcessor(my_cj)
opener = urllib.request.build_opener(m_handler)
res = opener.open('http://www.baidu.com')
#保存cookie到文件
my_cj.save()
print(res.read().decode())
```

LMPCookieJar:用法与MozillaCookieJar一样，保存后的内容格式不一样

加载使用保存后的cookie,加载方式和保存方式必须一致

```python
import urllib.request
from http import cookiejar
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36",
}
my_cookiejar = cookiejar.MozillaCookieJar(filename='baidu.txt')
my_cookiejar.load()#装载cookie
cookie_handler = urllib.request.HTTPCookieProcessor(my_cookiejar)
opener = urllib.request.build_opener(cookie_handler)
request = urllib.request.Request('http://www.baidu.com', headers=headers)
res = opener.open(request)
print(res.read().decode())
```

#### ProxyHandler处理器（代理设置）

谷歌浏览器的代理设置步骤：

设置-----》高级----》打开代理设置

电脑系统代理设置

控制面板中的Internet选项下的连接----》局域网设置

用云服务器找代理ip---------代理池

git clone  git@github.com:jhao104/proxy_pool.git需要用户名密码

git clone https://github.com/jhao104/proxy_pool.git直接用ip

redis安装： apt install redis-server

查询本地的ip是多少：ip138.com

创建一个代理handler

```python
import urllib.request

proxy = {
    'http': 'http://82.114.241.138:8080',
}

proxy_handler = urllib.request.ProxyHandler(proxies=proxy)

opener = urllib.request.build_opener(proxy_handler)

url = 'http://200019.ip138.com/'

res = opener.open(url)

print(res.read().decode())
```

requests包,动态代理ip

```python
import urllib.request
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36",
}

proxy_sever = requests.get('http://http://47.107.138.16:5010/get/', headers=headers).text
print(proxy_sever)

proxy = {
    'http': f'http://{proxy_sever}',
}

proxy_handler = urllib.request.ProxyHandler(proxies=proxy)

opener = urllib.request.build_opener(proxy_handler)

url = 'http://200019.ip138.com/'

res = opener.open(url)

print(res.read().decode())
```

#### user-agent的伪装

```python
from fake_useragent import UserAgent
import urllib.request

ua = UserAgent()#随机生成谷歌的用户代理
headers = {
    'user-agent': ua.chrome,
}

url = 'http://www.baidu.com'
request = urllib.request.Request(url=url, headers=headers)
res = urllib.request.urlopen(request)

print(res.read().decode('gbk'))
```

##### 简单的get与post请求

get请求：

```python
import requests

url = 'http://www.baidu.com/'

response = requests.get(url=url)

print(response.content)  # 将响应解码成Byte
print(response.content.decode())#Byte解码字符串
print(response.cookies) #响应cookie
```

爬虫模拟百度搜索

```python
import requests
from fake_useragent import UserAgent

ua = UserAgent()
headers = {
    'user-agent': ua.chrome,
}


def searchBaidu(params):
    searchdata = {
        'wd': params,
    }
    url = 'https://www.baidu.com/s?'
    # 用request去搜索百度，带参数自动为post
    response = requests.get(url=url, params=searchdata, headers=headers)
    print(response.text)  # 将响应解码成Byte


if __name__ == '__main__':
    pa = input('请输入搜索内容：')
    searchBaidu(pa)
```

post请求：有道的反爬机制：在url中加_o

```python
url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule'
```

模拟有道词典：

```python
import requests
from fake_useragent import UserAgent
import json

ua = UserAgent()
headers = {
    'user-agent': ua.chrome,
}


def youdao_fanyi(i):
    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'

    data = {
        "i": f"{i}",
        "from": "AUTO",
        "to": "AUTO",
        "smartresult": "dict",
        "client": "fanyideskweb",
        "salt": "15608651881610",
        "sign": "19efc0e04e2a75d0446a017ce1280431",
        "ts": "1560865188161",
        "bv": "8ad38d5814dfbafd0a7f3269d85c17bb",
        "doctype": "json",
        "version": "2.1",
        "keyfrom": "fanyi.web",
        "action": "FY_BY_REALTlME",
    }
    # 通过request  post来发起post请求
    res = requests.post(url=url, data=data, headers=headers)
    json_dict = json.loads(res.text)
    translateResult = json_dict.get('translateResult')[0][0]
    print(translateResult.get('tgt'))


if __name__ == '__main__':
    fanyi = input('请输入要翻译的内容：')
    youdao_fanyi(fanyi)
```

requests设置动态代理ip

```python
import requests
proxies = {
    "http": "httP://10.10.1.10:3128"
}
requests.get("http://example.org", proxies=proxies)
```



##### 认证

```python
import requests
auth = ('test', '123456')
res = requests.get(url='https://api.github.com/user', auth=auth)
print(res.text)
```

##### 把cookiejar转换成字典,存入

```python
res = requests.post(url=url, data=data, headers=headers)
cookie_dict = requests.utils.dict_from_cookiejar(res.cookie)
#保存cookie
with open('zmzrecookie.txt', 'wb') as fp:
    json.dump(cookie_dict,fp)
```

加载cookie文件及使用

```python
import requests
from fake_useragent import UserAgent
import json
headers = {
    'user-agent': UserAgent().firefox
}
url = 'http://www.zmz2019.com/user/user'
#加载cookie
zmz_cookie = json.load(open('zmzrecookie.txt','r'))
print(zmz_cookie)
#使用cookie
res = requests.get(url=url, headers=headers,cookies=zmz_cookie)
print(res.text)
```

##### ssl证书认证:verify=False可以跳过证书验证，正常请求

response = requests.get('http://www.12306.cn/mormhweb', verify=False)

```python
import requests
from fake_uaeragent import UserAgent
headers = {
    'user-agent': UserAgent().chrome
}
#不检查服务器的证书验证
res = requests.get('http://www.baidu.com/',headers=headers, verify=False)
print(res.text)
```



##### session对象：

保存cookiejar

```python
import requests
from fake_useragent import UserAgent
import json
import pickle
headers = {
    'user_agent':UserAgent().chrome
}
url = 'http://www.zmz2019.com/User/Login/ajaxLogin'
data = {
    'account': '账号'，
    'password': '请求的加密密码'，
    'remember': '1',
    'url_back': 'http://www.zmz2019.com/'
}
#创建一个session对象
session = requests.session()
#用session去post请求
res = session.post(url=url, data=data, headers=headers)
print(res.text)
res_dict = json.loads(res.text)
print(res.cookies)
#保存二进制cookiejar对象
with open('zmzcookiejar', 'wb') as fp:
	pickle.dump(session, fp)
url = 'http://www.zmz2019.com/user/user'
res = session.get(url=url, headers=headers)
print(res.text)
```

使用session的cookiejar对象

```python
import pickle
import requests
headers = {
    'user_agent':UserAgent().chrome
}
url = 'http://www.zmz2019.com/user/user'
#从文件中加载cookiejar对象
cookiejar = pickle.load(open('zmzcookiejar', rb))
res = session.get(url=url, headers=headers)
print(res.text)
```

##### 爬取策略

1.深度爬取

```python
import requests
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'
}


# 解析网页
def parse_html(url):
    res = requests.get(url, headers=headers)
    # 返回正常
    if res.status_code == 200:
        html = res.text
    else:
        html = ''
    return html


# 取出url
def get_son_url(html):
    # 子url的正则
    url_re = '<a.*?href="(.*?)".*?</a>'
    url_list = re.findall(url_re, html)
    # 返回子url的列表
    return url_list


# 爬取
def deep_spider(url, deep_level):
    # 对比当前的层级，如果层级大于要爬的，就返回
    if url_dict[url] > deep_level:
        return
    print('\t' * url_dict[url], f'当前的层级为：{url_dict[url]} url:{url}')
    html = parse_html(url)
    # 如果是空，中断当前爬取
    if not html:
        return
    # 子url的列表
    son_url_list = get_son_url(html)
    # 循环每一个url
    for son_url in son_url_list:
        if son_url.startswith('http'):
            # 判断是否在url中
            if (son_url in url_dict) or son_url.endswith('.exe'):
                return
            # 保存son_url字典
            url_dict[son_url] = url_dict[url] + 1
            # 重新调取函数，再爬
            deep_spider(son_url, deep_level)


if __name__ == '__main__':
    # 创建一个字典保存爬过的url
    url_dict = {}
    url = 'https://www.163.com/'
    url_dict[url] = 1
    # 爬取三层
    deep_spider(url, 3)
```

2.广度爬取

```python
import requests
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'
}


# 解析网页
def parse_html(url):
    res = requests.get(url, headers=headers)
    # 返回正常
    if res.status_code == 200:
        html = res.text
    else:
        html = ''
    return html


# 取出url
def get_son_url(html):
    # 子url的正则
    url_re = '<a.*?href="(.*?)".*?</a>'
    url_list = re.findall(url_re, html)
    # 返回子url的列表
    return url_list


# 爬取
def deep_spider(url, deep_level):
    url_list = []
    url_list.append(url)
    #当url_list有内容时会一直爬
    while len(url_list):
        #取出一个url
        url = url_list.pop()
        # 对比当前的层级，如果层级大于要爬的，就返回
        if url_dict[url] > deep_level:
            continue
        print('\t' * url_dict[url], f'当前的层级为：{url_dict[url]} url:{url}')
        try:
            html = parse_html(url)
        except:
            continue
        # 如果是空，中断当前爬取
        if not html:
            continue
        try:
            # 子url的列表
            son_url_list = get_son_url(html)
        except:
            continue
        # 循环每一个url
        for son_url in son_url_list:
            if son_url.startswith('http'):
                # 判断是否在url中
                if (son_url in url_dict) or son_url.endswith('exe'):
                    continue
                # 保存son_url字典
                url_dict[son_url] = url_dict[url] + 1
                # 把当前页面中的url加到列表中
                url_list.append(son_url)


if __name__ == '__main__':
    # 创建一个字典保存爬过的url
    url_dict = {}
    url = 'https://www.163.com/'
    url_dict[url] = 1
    # 爬取三层
    deep_spider(url, 3)
```

##### 页面的解析和数据的提取

安装包：pip install bs4

```python
import re

from bs4 import BeautifulSoup

doc_html = '''
<!DOCTYPE html>
<html>
    <head>
        <title>The Dormouse's story</title>
    </head>
    <body>
        <p class="title">
            One
            <b>The Dormouse's story</b>
        </p>
        <p class="story">
            <a class="box" href="elsie" id="link1">1111</a>bbbb
            <a class="sister" href="lacie" id="link2">2222</a>
            <a class="sister" href="tillie" id="link3">3223</a>
        </p>
        <p class="story">aaa</p>
    </body>
</html>
'''

# fp = open('bs4demo.html', encoding='utf-8')

soup = BeautifulSoup(doc_html, 'lxml')
# print(soup)
# print(type(soup))  # <class 'bs4.BeautifulSoup'>

# print(soup.prettify())  # 格式化输出

# Tag 标签
print(soup.head)
print(soup.head.title)
print(soup.head.name)  # head,标签名

print(soup.p)  # 只会得到第一个p
print()

# 属性
print(soup.p.attrs)  # {'class': ['title']},得到p节点的所有属性
print(type(soup.p.attrs))  # <class 'dict'>
print(soup.a.attrs)  # {'class': ['sister'], 'href': 'elsie', 'id': 'link1'},得到第一个a节点的所有属性
print(soup.a.attrs["class"])  # ['sister']
print(soup.a.attrs.get('href'))  # elsie
print(soup.a['href'])  # elsie
print()

# 文本
# string: 查找最里层元素（叶子节点）的内容
print(soup.p.string)  # None
print(soup.p.b.string)  # 'The Dormouse's story'

# text, get_text(): 获取节点内部的文本内容
print(soup.p.text)  # innerText, 节点内部的所有文本内容
print(soup.p.b.text)  # 'The Dormouse's story'
print(soup.p.b.get_text())  # 'The Dormouse's story'
print()

# 兄弟节点
# print(soup.a.next_sibling)  # 下一个节点（包括文本节点）
# print(soup.a.next_sibling.next_sibling)  # 下一个节点（包括文本节点）
# print(soup.a.previous_sibling)  # 上一个节点（包括文本节点）
print("================" * 3)

# find_all()
print(soup.find('p'))  # 查找到第一个p
print(soup.find_all('p'))  # 所有的p节点, 列表

# 可以使用正则
print(soup.find_all(re.compile('^p$')))

# 或运算： []
print(soup.find_all(['p', 'a']))  # 获取a标签和p标签

# 查找指定属性的标签
print(soup.find_all('a', attrs={'href':'elsie'}))  # 查找指定属性的元素
print(soup.find_all('a', href='elsie'))  # 查找指定属性的元素
print(soup.find_all('a', class_='sister'))  # 查找指定class属性
print(soup.find_all('a', class_='sister', limit=2))  # 前2个
print()

# 根据文本内容查找
print(soup.find_all('a', text="1111"))  # 查找内容为1111的元素，完全一样
print(soup.find_all('a', text=re.compile('22')))  # 查找指定正则匹配的元素
print('\n')


# select(): 选择器
print(soup.select('a'))  # 找到所有a标签
print(soup.select('a.box'))
print(soup.select('a#link3'))
print(soup.select('p > a#link3'))
print(soup.select('p a#link3'))
```



##### 示例：爬取前程无忧招聘岗位数量

```python
from bs4 import BeautifulSoup
import requests

def download(url):
    headers = {"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);"}
	response = requests.get(url, headers=headers)
	html = response.content.decode('gbk')
    
    soup = BeautifulSoup(html, 'lxml')
    # 获取岗位数量的多种查找方式
    # 方式1： 使用find_all
    jobnum = soup.find_all('div', class_='rt')
    print(jobnum[0].text)
    
    # 方式2： 使用select
    jobnum = soup.select('.rt')[0].string
	print(jobnum.strip())  # 去掉首尾空格

	# 方式3：正则匹配re
	# jobnum_re = '<div class="rt">(.*?)</div>'
	# jobnum_comp = re.compile(jobnum_re, re.S)
	# jobnums = jobnum_comp.findall(html)
	# print(jobnums[0])

download(url = "https://search.51job.com/list/000000,000000,0000,00,9,99,python,2,1.html?lang=c&stype=&postchannel=0000&workyear=99&cotype=99&degreefrom=99&jobterm=99&companysize=99&providesalary=99&lonlat=0%2C0&radius=-1&ord_field=0&confirmdate=9&fromType=&dibiaoid=0&address=&line=&specialarea=00&from=&welfare=")

```

##### 示例：爬取股票基金

```python
import urllib
from urllib import request
from bs4 import BeautifulSoup

stockList = []

def download(url):
    headers = {"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);"}
    request = urllib.request.Request(url, headers=headers)  # 请求，修改，模拟http.
    data = urllib.request.urlopen(request).read()  # 打开请求，抓取数据
    
    soup = BeautifulSoup(data, "lxml", from_encoding="gb2312")
    mytable = soup.select("#datalist")
    for line in mytable[0].find_all("tr"):
        print(line.get_text())  # 提取每一个行业
        print(line.select("td:nth-of-type(3)")[0].text) # 提取具体的某一个

if __name__ == '__main__':
    download("http://quote.stockstar.com/fund/stock_3_1_2.html")

```

#### Xpath：

安装：pip install lxml

导包：from lxml import etree

```python
import lxml
from lxml import etree

html = """
<html>
    <head>
        <title>The Dormouse's story</title>
    </head>
    <body>
        <p class="title">
            One
            <b>The Dormouse's story</b>
            <a class="box" href="elsie" id="link1">12</a>bbbb
        </p>
        <p class="story">
        	<b>HaHa</b>
            <a class="box" href="elsie" id="link1">1111</a>bbbb
            <a class="sister" href="lacie" id="link2">2222</a>
            <a class="sister" href="tillie" id="link3">3223</a>
        </p>
        <p class="story">
        aaa
        <b>story</b>
        <a class="box" href="elsie" id="link1">1111</a>bbbb
        </p>
    </body>
</html>
"""
mytree = etree.HTML(html)

root = mytree.xpath('/html')[0]
print(root)
print(lxml.etree.tostring(root).decode())  # tostring将xml转为二进制字符串
```

##### 选取节点：取出来的都是列表

```python
mytree = etree.HTML(html)
body = mytree.xpath('/html/body')[0]#选取html节点下的body子元素节点
body = myree.xpath('/html//p')[0]#选取html节点中的所有子节点或孙子。。。的p节点
body = myree.xpath('/html//p[1]')[0]#选取html节点中的所有子节点或孙子。。。的第一个p节点
for bodys in body:
	bodys.xpath('./a')[0]#选取bodys节点下的所有a标签
print(lxml.etree.tostring(body).decode())

```

#### 选取未知节点

```python
mytree = etree.HTML(html)
text = mytree.xpath('//*/@*')#选取所有节点的属性
tetx = mytree.xpath('//ul/*/text()')#选取ul下面的所有标签的文字
text = mytree.xpath('//p[position()=3]')#选取第三个
text = mytree.xpath('//p[position()>3]')#选取大于3的
text = mytree.xpath('//p[position()<=3]')#选取小于等于3的
```

##### 条件选取

```python
text = mytree.xpath('//p[a>100]/b/text()')#选取当前节点下的a标签值大于100的b标签的内容----可以有<=  >=
text = mytree.xpath('//a[@href]/text()')#选取所有a标签节点中有属性href的节点内容
text = mytree.xpath('//a[@href]="elsie"/text()')#选取所有a标签节点中有属性href为elsie的节点内容
p = mytree.xpath('//p[2]/@class')#选取第二个p标签class属性的值
boo = mytree.xpath('//p/a/text() | //p/b/text()')#选取p标签下的a标签的值和b标签的值
```

#### selenium使用

##### get(url)：打开URL----测试操作

```python
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
def openURL():
    driver = webdriver.Chrome('可以指定chromedriver绝对路径')
    driver.get("http://www.baidu.com")
    # 打印页面的html代码
    print(driver.page_source)
    # 根据name来查找元素
    wd = driver.find_element_by_name('wd')
    # 根据id来查找元素
    wd = driver.find_element_by_id('kw')
    wd.send_keys('菜虚鲲', Keys.ENTER)#在输入框中输入菜虚鲲，并按下回车
    driver.find_element_by_xpath('//a[@wdfield="word"][3]').click()  #通过xpath规则找到a标签，然后点击
    print(driver.window_handles) #将所有窗口的句柄打印
    print(driver.current_window_handle)# 打印当前窗口的句柄
    driver.switch_to.window('CDwindow-B463E80BB1A71F3D95E5EBEF7689FC06')#将打印出来的句柄复制，然后切换到该句柄的窗口
    driver.execute_script('window.scrollBy(0, 1000)')#滚动1000像素，scrollBy表示当前位置
    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')#滚动到界面最下方
```

#### 线程，进程和协程

##### 1，协程是什么

协程，又称微线程，纤程。英文名Coroutine，是一种用户态的轻量级线程。

协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方（线程调度时候寄存器上下文及栈等保存在内存中），在切回来的时候，恢复先前保存的寄存器上下文和栈。因此：

协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。

子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。

所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。

子程序调用总是一个入口，一次返回，调用顺序是明确的。而协程的调用和子程序不同。

协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。

##### 协程的优点缺点：

无需线程上下文切换的开销
无需原子操作锁定及同步的开销
"原子操作(atomic operation)是不需要synchronized"，所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。原子操作可以是一个步骤，也可以是多个操作步骤，但是其顺序是不可以被打乱，或者切割掉只执行部分。视作整体是原子性的核心。
方便切换控制流，简化编程模型
高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。如Nginxhttps://blog.csdn.net/shootyou/article/details/6093562
缺点：

无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。

进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序

```python
import gevent
from gevent import monkey

gevent.monkey.patch_all()#调用此方法时，IO密集型请求时会自动进行切换，其他的串行

import requests


def fun(arg):
    print(arg, '开始运行')
    res = requests.get('http://www.baidu.com/')
    print(res.text)
    print(arg, '结束运行')


if __name__ == '__main__':
    g_list = []
    for i in range(1, 10):
        g = gevent.spawn(fun, arg=f'协程{i}')  # 创建协程
        g_list.append(g)
    gevent.joinall(g_list)  # 运行，当程序等待，自动会切换到其他子线程
```

##### 进程

注意:创建子进程其实是对主进程进行拷贝，进程之间相互独立，访问的全局变量不是同一个，所以进程之间不共享全局变量

```python
import multiprocessing
import time


def fun1(name):
    print('进程开始运行', multiprocessing.current_process().name)
    time.sleep(20)
    print('进程结束运行', name)


if __name__ == '__main__':
    p_list = []
    for i in range(1, 5):
        p = multiprocessing.Process(target=fun1, args=(f'CCTV{i}',))
        p.daemon = True  # 设置开启守护进程，当主进程结束时，子进程会被杀掉
        p.start()  # 开启进程
        p.join()  # 只能等待上一个进程完成，才能执行下一个
        p_list.append(p)

    for p in p_list:
        p.join()  # 一起等待主进程完成
```

子进程与主进程的关系

```python
import os
import time
from multiprocessing import Process


class MyProcess(Process):
    def __init__(self, name):
        super().__init__()  # 调用父类的init，完成初始化
        self.name = name  # 传入进程名称

    def run(self):
        print('进程开始运行', self.name)
        print(os.getpid())
        print("父进程pid", os.getppid())
        time.sleep(5)
        print('进程结束', self.name)


if __name__ == '__main__':
    print("主进程pid", os.getpid())
    p1 = MyProcess('李彬彬')
    p2 = MyProcess('范冰冰')

    p1.start()
    p2.start()
```

进程锁：一般用在磁盘文件，在内存变量中进程之间相互独立，访问的全局变量不是同一个，所以进程之间不共享全局变量

```python
import multiprocessing

num = 1

lock = multiprocessing.Lock()  # 实例化对象


def add():
    print('进程在运行', multiprocessing.current_process().name)
    global num
    with lock:  # 开启进程锁
        for i in range(1, 100000):
            num += 1
    print(num)
    return num


if __name__ == '__main__':
    for i in range(1, 6):
        p = multiprocessing.Process(target=add)
        p.start()
```

信号量：

```python
import multiprocessing
import time


def fun(sem):
    with sem:
        print('当前进程正在运行', multiprocessing.current_process(), sem)
        time.sleep(5)
        print('进程结束', multiprocessing.current_process(), sem)


if __name__ == '__main__':
    sem = multiprocessing.Semaphore(3)  # 同时运行三个进程
    for i in range(1, 11):
        p = multiprocessing.Process(target=fun, args=(sem,))
        p.start()

    print('当前运行的进程数量：', multiprocessing.active_children())
```

线程池：

```python
import time

import threadpool
from threadpool import ThreadPool
import threading


def fun(arg):
    print('线程正在运行', threading.current_thread().name)  # 线程的属性
    time.sleep(5)
    print('线程结束运行', threading.current_thread().name)
    return arg


def cb(requst, result):
    print('返回的结果', result)


if __name__ == '__main__':
    arg_list = ['王建林', '马云', '马化腾', '赵薇']
    requests = threadpool.makeRequests(fun, args_list=arg_list, callback=cb)
    pool = threadpool.ThreadPool(3)
    for req in requests:
        pool.putRequest(req)  # 把线程放在线程池中

    pool.wait()  # 等待线程全部完成
```

进程池与线程池类似

```python
import time
import multiprocessing


def fun(arg):
    print('线程正在运行', multiprocessing.current_process().name)  # 进程的属性
    time.sleep(5)
    print('线程结束运行', multiprocessing.current_process().name)
    return arg


def cb(requst, result):
    print('返回的结果', result)


if __name__ == '__main__':
    arg_list = ['王建林', '马云', '马化腾', '赵薇']

    pool = multiprocessing.Pool(3)
    for arg in arg_list:
        # pool.apply(func=fun, args=(arg, ))  # 同步, 等待上一个进程完成才运行
        pool.apply_async(func=fun, args=(arg,), callback=cb)  # 异步

    pool.close()
    pool.join()
```

##### 什么是GIL

即全局解释器所（[global interpreter lock](https://www.baidu.com/link?url=wcyORPNvihJlWYZsd97z3F3g6xZfs09N64Jc56POq3P7r3h5vIxVFcjGiZjosC6g3jhmBh13DS36rKld5eBfWq&wd=&eqid=aa7605a200005349000000025aa0f4fb)），每个线程在执行时候都需要先获取GIL，保证同一时刻只有一个线程可以执行代码，即同一时刻只有一个线程使用CPU，也就是说多线程并不是真正意义上的同时执行。

我们改如何解决GIL锁的问题呢?

1.更换cpython为jpython(不建议)

2.使用多进程完成多线程的任务

3.在使用多线程可以使用c语言去实现

 什么时候会释放Gil锁?

1 遇到像 i/o操作这种 会有时间空闲情况 造成cpu闲置的情况会释放Gil
2 会有一个专门ticks进行计数 一旦ticks数值达到100 这个时候释放Gil锁 线程之间开始竞争Gil锁(说明:ticks这个数值可以进行设置来延长或者缩减获得Gil锁的线程使用cpu的时间)

互斥锁和Gil锁的关系

Gil锁  : 保证同一时刻只有一个线程能使用到cpu

互斥锁 : 多线程时,保证修改共享数据时有序的修改,不会产生数据修改混乱



#### matplotlib

1.简单直线

```python
from matplotlib import pyplot as plt

plt.plot([1, 2], [3, 4])#起点为1，3  终点为 2，4的直线

plt.show()
```

2.简单的折线

```python
from matplotlib import pyplot as plt

plt.plot([1, 2, 3], [3, 4, 3], '--')#起点为1，3和2，4和3，3的折线线, 线条为虚线当为--*时为虚线加转角带*号，也可以为>, ^等
#当'.'时只显示坐标点
plt.text(0.8, 3, '<3>')在点0.8，3处显示字符串
plt.show()
```

3.柱状图

```python
from matplotlib import pyplot as plt

#plt.bar([1, 2], [3, 4])#柱状图1，高度为3：柱状图3，高度为4
plt.bar([1], [3], label='boy')#显示图例为boy
plt.legend()
plt.show()
plt.savefig()# 将图保存在当前目录中
```

4.饼状图

```python
from matplotlib import pyplot as plt

x = [1, 4, 5, 6, 9, 10, 12]#所占比例大小
label = ['picture', 'desk', 'cup', 'chair', 'handle', 'glass', 'wire']#为图例
exp = [0.9, 0, 0, 0, 0, 0, 0]  # 每块离开圆心的距离
plt.pie(x=x, labels=label, autopct="%1.2f%%"，shadow=True, , explode=exp)#显示所占的比例,shadow=True表示显示阴影
plt.legend()#图例
plt.show()
```

os.path.splitext(url)[1]-----拆分扩展名，并取出扩展名

#### 抓包过程

1.安装Fiddler

2.在tool中的Options--》HTTPS--》全部勾选后点击Actions--》点击Trust  Root  Certificate就可以抓包了

##### 3.开热点给手机连接，然后抓包手机数据

。在tool中的Options--》Connections，勾选Allow remote computers to connect --》ok

。再cmd中netsh wlan set hostednetwork mode=allow ssid=mytest key=12345678，ssid为热点名  ，key为密码

#### Scrapy架构图

~~~python


```python
Scrapy主要包括了以下组件：
	Scrapy Engine(引擎): 
        负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。

	Scheduler(调度器): 
        它负责接受`引擎`发送过来的Request请求，并按照一定的方式进行整理排列，入队，当`引擎`需要时，交还给`引擎`。

	Downloader（下载器）：
    	负责下载`Scrapy Engine(引擎)`发送的所有Requests请求，并将其获取到的Responses交还给`Scrapy Engine(引擎)`，由`引擎`交给`Spider`来处理，

	Spider（爬虫）：
    	它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给`引擎`，再次进入`Scheduler(调度器)`，

	Item Pipeline(管道)：
    	它负责处理`Spider`中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.

	Downloader Middlewares（下载中间件）：
		你可以当作是一个可以自定义扩展下载功能的组件。

	Spider Middlewares（Spider中间件）：
    	你可以理解为是一个可以自定扩展和操作`引擎`和`Spider`中间`通信`的功能组件（比如进入`Spider`的Responses和从`Spider`出去的Requests）
        
```
~~~

安装

```python
Scrapy的安装介绍
	Scrapy框架官方网址：http://doc.scrapy.org/en/latest
	Scrapy中文维护站点：http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html
	
安装：
	pip install pywin32
	pip install scrapy
其它安装方式:   
	1、安装wheel
		pip install wheel
    2、安装lxml
		pip install lxml
    3、安装pyopenssl
		pip install pyopenssl
    4、安装Twisted
		需要我们自己下载Twisted，然后安装。这里有Python的各种依赖包。选择适合自己Python以及系统的Twisted版本：https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
		# 3.6版本（cp后是python版本）
         pip install Twisted-18.9.0-cp36-cp36m-win_amd64.whl
		
    5、安装pywin32
		pip install pywin32
    6、安装scrapy
        pip install scrapy
        
安装后，只要在命令终端输入scrapy来检测是否安装成功
启动爬虫：
scrapy crawl dangspider---爬虫名称
```

Scrapy运行流程大概如下：

引擎(Scrapy)从调度器(Scheduler)中取出一个链接(URL)用于接下来的抓取
引擎把URL封装成一个请求(Request)传给下载器(Downloader)
下载器把资源下载下来，并封装成应答包(Response)
爬虫(Spiders)解析Response
解析出实体（Item）,则交给实体管道(Pipeline)进行进一步的处理

解析出的是链接（URL）,则把URL交给调度器等待抓取

中间件

下载器中间件(Downloader Middlewares)：
位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
爬虫中间件(Spider Middlewares)：
介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。
调度中间件(Scheduler Middewares)：

介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应

#### 使用爬虫可以遵循以下步骤：

1. 创建一个Scrapy项目

2. 定义提取的Item

3. 编写爬取网站的 spider 并提取 Item

4. 编写 Item Pipeline 来存储提取到的Item(即数据)


1. 新建项目(scrapy startproject)

   。scrapy startproject meiju

创建爬虫程序

cd meiju
scrapy genspider meijuSpider meijutt.com

其中：
	meijuSpider为爬虫文件名
	meijutt.com为爬取网址的域名

创建Scrapy工程后, 会自动创建多个文件，下面来简单介绍一下各个主要文件的作用：

```python
scrapy.cfg：
	项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）
items.py：
	设置数据存储模板，用于结构化数据，如：Django的Model
pipelines：
	数据处理行为，如：一般结构化的数据持久化
settings.py：
	配置文件，如：递归的层数、并发数，延迟下载等
spiders：
	爬虫目录，如：创建文件，编写爬虫规则
    
注意：一般创建爬虫文件时，以网站域名命名
```

禁止cookies

COOKIES_ENABLED = False

减少下载超时

DOWNLOAD_TIMEOUT = 15,其中15是设置的下载超时时间

设置延迟

DOWMLOAD_DELY=3,设置延迟下载可以避免被发现

- scrapy shell url 查看网页，不过这种方式可能对于要请求头的网页不行，对于一般的网页还是可以的

- scrapy view shell 用来查看动态加载的网页，如果查看的网页用了动态加载，那么用这个命令行打开的网页就是不完整的，肯定缺少了什么

#### 用start来启动项目

```python
import scrapy.cmdline
scrapy.cmdline.execute('scrapy crawl newsspider'.split())
```

用简单scrapy爬取和处理

```python
import scrapy


class NewsspiderSpider(scrapy.Spider):
    name = 'newsspider'#建立的那个爬虫
    allowed_domains = ['sina.com.cn']#要爬取的域名
    start_urls = ['http://roll.news.sina.com.cn/news/gnxw/gdxw1/index_1.shtml'] #要爬取的地址

    def parse(self, response):
        news_list = response.xpath('//ul[@class="list_009"]/li')#直接用xpath解析
        # print(news_list)
        # print(response.url)
        # print(response.headers)
        # print(response.request.headers)
        for news in news_list:
            title = news.xpath('./a/text()').extract_first()  # 取列表中的第一条数据的data
            time = news.xpath('./span/text()')[0].extract()  # 与上面作用一样
            print(time, title)
```

设置用户代理

在setting文件中

```python
from fake_useragent import UserAgent
USER_AGENT = UserAgent().chrome
ROBOTSTXT_OBEY = False
ITEM_PIPELINES = {
   'sinanews.pipelines.SinanewsPipeline': 300,
}
```

在items文件中：作用，将其转化为字典格式

```python
import scrapy


class SinanewsItem(scrapy.Item):
    # define the fields for your item here like:
    time = scrapy.Field()
    title = scrapy.Field()
```



数据处理管道的item是通过spider传入的

传入后对其进行处理，并再次调用spider中的生成器，再返回

```python
import scrapy
from sinanews.items import SinanewsItem


class NewsspiderSpider(scrapy.Spider):
    name = 'newsspider'
    allowed_domains = ['sina.com.cn']
    start_urls = ['http://roll.news.sina.com.cn/news/gnxw/gdxw1/index_1.shtml']
	page = 1
    url = 'http://roll.news.sina.com.cn/news/gnxw/gdxw1/index_{}.shtml'
    def parse(self, response):
        news_list = response.xpath('//ul[@class="list_009"]/li')
        # print(news_list)
        # print(response.url)
        # print(response.headers)
        # print(response.request.headers)
        for news in news_list:
            title = news.xpath('./a/text()').extract_first()  # 取列表中的第一条数据的data
            time = news.xpath('./span/text()')[0].extract()  # 与上面作用一样

            itmes = SinanewsItem()
            itmes['title'] = title
            itmes['time'] = time

            yield itmes  # 在这可以声明这个为生成器函数
        if self.page < 5:
            self.page += 1  # 每次页数加一
            url = self.url.format(self.page)  # 用当前的页码数，格式化
            yield scrapy.Request(url=url, callback=self.parse)  # 重新请求url，再调用自身，callback为钩子函数
```

在使用生成器的时候，内部调用的原理

```python
def process_item(item):
    print(item)
    return item

def parse():
    for i in range(1,10):
        item = {'name':'马飞飞','age':i}
        yield item

if __name__ == '__main__':
myparse = parse()
for item in parse():
    process_item(item)
```
管道中的内部处理简单过程：存入数据库

```python
import pymysql


class SinanewsPipeline(object):
    # 爬虫初始化
    def __init__(self):
        print('this is open spider')

    # 爬虫打开时的调用
    def open_spider(self, spider):
        self.db = pymysql.connect(host='127.0.01', user='root', password='123456')

    # 处理item函数
    def process_item(self, item, spider):
        title = item['title']
        time = item['time']
        sql = f"insert into news(title, time) values ('{title}', '{time}')"
        print(sql)
        # 开启事务
        self.db.begin()
        try:
            self.cursor.execute(sql)
        except Exception as e:
            self.db.rollback()  # 如果出错回滚
        return item

    def close_spider(self, spider):
        print('this is close spider')
        self.cursor.close()
        self.db.close()
```

第二种scrapy方法

```python
import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor


def my_process_value(value):
    print(value)
    # value = value + '?'  # 对url进行处理
    if not value.startswith('http'):
        value = 'http://sina.com.cn/' + value
    return value


class NewsspiderSpider(CrawlSpider):
    name = 'newsspider'
    allowed_domains = ['sina.com.cn']
    start_urls = ['http://roll.news.sina.com.cn/news/gnxw/gdxw1/index_1.shtml']  # 请求url

    rules = {Rule(
        link_extractor=LinkExtractor(
            allow=(r'index_\d+\.shtml',),
            restrict_xpaths=('//div[@class="listBlk]"/table',),  # 限制只爬取，特定xpath中的url
            attrs=('href', 'href2'),  # 匹配url属性
            process_value=my_process_value  # 可以用函数处理url
        ),  # 链接提取器  allow只匹配allow元组中的正则
        callback='myparse',  # 回调函数
        follow=True  # 爬取页面时会检测未点击的url内容中是否有能匹配的url
    )}

    def myparse(self, response):
        print(response)
```

日志的开启：

在setting中加入：LOG_FILE = 'sina.log'-----日志文件名

##### 反反爬的策略

禁用Cookies

设置延迟下载：DOWNLOAD_DELAY = 3

使用ip代理池

使用Crawlera：专用于爬虫的代理组件，正确配置和下载中间件后，项目的所有的request都是通过crawlera发出

```python
DOWNLOADER_MIDDLEWARES = {
    'scrapy_crawleraMiddleware': 600
}
```

自定义中间件，使用ip代理池

在settings中注册：

```python
DOWNLOADER_MIDDLEWARES = {
    'zmz.middlewares.ProxyMiddleware': 543,#指定zmz.middlewares下类名为ProxyMiddleware的中间件，543为中间件权重,数值越小权重越大，优先执行权重大的
}
```

在中间件文件中设置代理

```python
class ProxyMiddleware():
    def process_request(self, requst, spider):
        request.meta['proxy'] =  "http://175.23.42.41:8080" #设置代理ip
        return request
```

注意：request.META 是一个Python字典，包含了所有本次HTTP请求的Header信息，比如用户IP地址和用户Agent（通常是浏览器的名称和版本号）。 注意，Header信息的完整列表取决于用户所发送的Header信息和服务器端设置的Header信息

scrapy中post请求的使用：scrapy.FormRequest

```python
import json
import scrapy


class BaiduspiderSpider(scrapy.Spider):
    name = 'baiduspider'
    allowed_domains = ['fanyi.baidu.com']
    start_urls = ['http://fanyi.baidu.com/']

    # 重写了start_requests 方法，打开爬虫时，会调用这个方式，并且只会调用一次
    def start_requests(self):
        url = 'http://fanyi.baidu.com/sug'

        data = {
            'kw': 'glass',
        }
        # 这只是一个表单的请求，一个post方法, dont_filter=True表示过滤相同的url请求
        yield scrapy.FormRequest(url=url, formdata=data, dont_filter=True)

    def parse(self, response):
        print(response.text)
        res_dict = json.loads(response.text)
        print(res_dict)
```

scrapy中cooki的使用

```python
import scrapy

class LoginspiderSpider(scrapy.Spider):
    name = 'loginspider'
    allowed_domains = ['zmz2019.com']
    start_urls = ['http://zmz2019.com/']
    cookies = {
        'UM_distinctid':'16ad34c98072b7-0903581c387da-404b0d2c-15f900-16ad34c98093f9',
        'cps3':'163%2F1560906374%3Bxmyp%2F1560850183%3Bvip%2F1560850219%3Bvmall%2F1560850384%3Bsuning%2F1560851593',
        'ctrip':'ctrip2%2F1560906374',
        'last_announ':'1560743623',
        'CNZZDATA1275589337':'2057352492-1560847268-%7C1560911130',
        'CNZZDATA1254180690':'937014765-1558319010-%7C1561525575',
        'PHPSESSID':'k17sljvv0a72oosi8bjp0do4e5',
        'GINFO':'uid%3D3619515%26nickname%3Dmamamiya%26group_id%3D1%26avatar_t%3Dhttp%3A%2F%2Ftu.jstucdn.com%2Fftp%2Favatar%2F2017%2F0408%2F059d5683e2d2b0a8c4b5c20e1bd941a3_t.png%26main_group_id%3D39%26common_group_id%3D59',
        'GKEY':'8ccc7823271609b3c51b96ea58a06fde'
    }

    def start_requests(self):
        url = 'http://www.zmz2019.com/user/user'
        yield scrapy.Request(url=url, cookies=self.cookies, callback=self.parse)  #使用cookie登录

    def parse(self, response):
        print(response.text)
```
scrapy中使用selenium，在中间件中使用

```python
import time

from scrapy import signals
from selenium import webdriver
from scrapy.http import HtmlResponse

options = webdriver.ChromeOptions()
# 设置为开发者模式，防止被各大网站识别出来使用了Selenium
options.add_experimental_option('excludeSwitches', ['enable-automation'])

#登录中间键，所有的请求都会经过这个中间键
class ZhihuLoginMiddleware():
    def process_request(self,request,spider):
        if 'signup' in request.url or 'signin' in request.url: #如果是登录请求，就开启selenium
            spider.browser = webdriver.Chrome(chrome_options=options)
            spider.browser.get(request.url)
            time.sleep(1)
            spider.browser.find_element_by_xpath('//div[@class="SignContainer-switch"]/span').click()
            time.sleep(1)
            spider.browser.find_element_by_name('username').send_keys('18058766787')
            spider.browser.find_element_by_name('password').send_keys('r67543452')
            time.sleep(1)
            spider.browser.find_element_by_xpath('//form[@class="SignFlow"]/button').click()
            time.sleep(3)
            spider.cookies = spider.browser.get_cookies()  #获取登录后的cookie

            html = spider.browser.page_source  #登录后的html
            url = spider.browser.current_url  #当前页面的url
            return HtmlResponse(url=url, body=html, encoding='utf-8')
        else:
            for cookie in spider.cookies:
                request.cookie['name'] = cookie['name']
```

#### 分布式爬虫

定义：分布式爬虫用一个共同的爬虫程序，同时部署到多台电脑上运行，这样可以 提高爬虫速度，实现分布式爬虫。——极光爬虫

安装Scrapy-Redis

在github中下载分布式架构，地址：<https://github.com/rmax/scrapy-redis>

使用命令：git clone https://github.com/rmax/scrapy-redis.git到本地地址

将redis配置为远程连接

在启动文件下打开redis.windows-service.conf 将bind 127.0.0.1改为bind 0.0.0.0

在cmd下netstat -ant 查看6379端口是否为0.0.0.0

进入到redis输入命令行;config set requirepass  999        将密码改为999

这密码只能本次使用当电脑重启时会变成配置文件中的密码

使用auth  999 来登录

修改redis的永久密码

修改redis.windows-service.conf文件中的：

将#requirepass foobared。设置密码的方法就是去掉注释的#，把foobared替换成自己的密码即可，例如将密码设置为123456

数据存入数据库---在pipelines中进行操作

```python
import pymysql

class insertPipeline(object):
    #爬虫初始化
    def __init__(self):
        pass
    #爬虫打开的时候会调用
    def open_spider(self, spider):
        self.db = pymysql.connect(host='127.0.0.1', user='root', password='rock1204', database='sinanews')
        self.cursor = self.db.cursor()
    #处理itme函数
    def process_item(self, item, spider):
        title = item['title']
        sql = f"insert into news(title,) values('{title}',)"
    	self.db.begin()
        try:
            self.cursor.execute(sql)
        except Exception as e:
            self.db.rollback()#出错回滚
        self.db.commit()
        return item
    #爬虫关闭时，调用
    def close_spider(self, spider):
        self.cursor.close()
        self.db.close()
```







数据的持久化

```python

import json
import redis
import pymysql

def main():

    redis_cli = redis.Redis(host='localhost',port=6379)

    mysqldb = pymysql.connect(host='localhost', user='root', password='123456',database='xiaohua')

    while True:
        source,data = redis_cli.blpop(['xiaohua_spider:items'])
        item = json.loads(data)
        print(item)
        try:
            cursor = mysqldb.cursor()
            sql = "insert into pics(name, img) values ('%s','%s')" %(item.get('name'), item.get('picture_url'))
            cursor.execute(sql)
            mysqldb.commit()
            cursor.close()

        except mysqldb.Error as e:
            mysqldb.rollback()
            print(e)



if __name__ == '__main__':
    main()
```

分页爬取

```python
import scrapy

from ximalaya1.items import Ximalaya1Item


class XimalayaspiderSpider(scrapy.Spider):
    name = 'ximalayaspider'
    allowed_domains = ['ximalaya.com']
    start_urls = ['https://www.ximalaya.com/renwen/reci433/']

    page = 1
    url = 'https://www.ximalaya.com/renwen/reci433/p{}/'

    def parse(self, response):
        book_list = response.xpath('//div[@class="content"]/ul/li')
        for books in book_list:
            topic = books.xpath(
                './div[@class="album-wrapper  sm _qie"]/a[@class="album-title line-1 lg bold _qie"]/@title')
            topic = topic.extract_first()
            book_url = books.xpath(
                './div[@class="album-wrapper  sm _qie"]/a[@class="album-title line-1 lg bold _qie"]/@href')
            book_url = book_url.extract_first()
            name = books.xpath('./div[@class="album-wrapper  sm _qie"]/a[@class="album-author _qie"]/text()')
            name = name.extract_first()
            item = Ximalaya1Item()
            item['topic'] = topic
            item['name'] = name
            item['book_url'] = book_url
            yield item
        if self.page < 5:
            self.page = self.page + 1
            url = self.url.format(self.page)
            yield scrapy.Request(url=url, callback=self.parse)
```